{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Q4duf-QUpaF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca3QT6a2kOzM"
      },
      "source": [
        "**LIMPIEZA DE DATOS DE PRODUCCION PARA PODER INCORPORAR EN EL ANALISIS DE VENTAS**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_nZvT-6kOc6"
      },
      "outputs": [],
      "source": [
        "df_produccion=pd.read_excel('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1137mfxKkOLo"
      },
      "outputs": [],
      "source": [
        "df_produccion = df_produccion.add_prefix(\"produccion_\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2TW77vikkCD"
      },
      "outputs": [],
      "source": [
        "df_produccion.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwoQyKNfknCu"
      },
      "outputs": [],
      "source": [
        "# Limpiar espacios al inicio y final de todas las categorías\n",
        "df_produccion['produccion_Cat. Producto'] = df_produccion['produccion_Cat. Producto'].str.strip()\n",
        "print(df_produccion['produccion_Cat. Producto'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsN7k_Wbksf7"
      },
      "outputs": [],
      "source": [
        "#Se eliminan los productos que no son de interes en ventas o inventario.\n",
        "categorias_eliminar = ['TRUCHA ENTERA',  'OVAS'\n",
        " ,'RETAZO DE TRUCHA AHUMADA']\n",
        "df_produccion = df_produccion[~df_produccion['produccion_Cat. Producto'].isin(categorias_eliminar)]\n",
        "\n",
        "# Verificar\n",
        "print(df_produccion['produccion_Cat. Producto'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAnW4UNXm0sd"
      },
      "outputs": [],
      "source": [
        "df_produccion.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "829dkdn8dxRz"
      },
      "source": [
        "**LIMPIEZA DE DATOS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oOdsKMpUzsi"
      },
      "outputs": [],
      "source": [
        "df_ventasFAC=pd.read_excel('/content/drive/MyDrive/Capstone/GM VENTAS FAC_sept2025.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kw4GrVrdVZjX"
      },
      "outputs": [],
      "source": [
        "df_ventasFAC.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0P4_hePDVeAZ"
      },
      "outputs": [],
      "source": [
        "# Crear copia\n",
        "df_ventasFAC_limpio = df_ventasFAC.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ck7q2FUtVhuf"
      },
      "outputs": [],
      "source": [
        "df_ventasFAC_limpio = df_ventasFAC_limpio[['Fecha', 'Nombre', 'Cat. Producto', 'Cantidad', 'Precio', '% Descuento', 'Total', 'Estado']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xm5JTW6OVotg"
      },
      "outputs": [],
      "source": [
        "df_ventasFAC_limpio.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2fgcaFIVsIN"
      },
      "outputs": [],
      "source": [
        "#Primero verificamos que son los blancos en Cat.Producto\n",
        "# Ver las filas donde Cat. Producto está vacío\n",
        "df_ventasFAC_limpio[df_ventasFAC_limpio['Cat. Producto'].isna()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15wFvpuRV-Je"
      },
      "outputs": [],
      "source": [
        "#Las filas que estan en blanco Cat. Producto son movimientos contables y no afectan en cantidades de venta de prodcuto, Se van a eliminar.\n",
        "# Eliminar ambos: NaN y cadenas vacías\n",
        "df_ventasFAC_limpio = df_ventasFAC_limpio[df_ventasFAC_limpio['Cat. Producto'].notna() & (df_ventasFAC_limpio['Cat. Producto'] != '')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4DaY4FLWUUB"
      },
      "outputs": [],
      "source": [
        " print(df_ventasFAC_limpio['Cat. Producto'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmw8b72YWinJ"
      },
      "outputs": [],
      "source": [
        "# Limpiar espacios al inicio y final de todas las categorías\n",
        "df_ventasFAC_limpio['Cat. Producto'] = df_ventasFAC_limpio['Cat. Producto'].str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIv6bCJgWorW"
      },
      "outputs": [],
      "source": [
        " print(df_ventasFAC_limpio['Cat. Producto'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAa2eMLwWq4I"
      },
      "outputs": [],
      "source": [
        "#Se eliminan los productos que no son de interes en ventas o inventario.\n",
        "categorias_eliminar = ['DIP Trucha Ahumada', 'OVAS', 'ENVIOS DE PRODUCTOS',\n",
        "                       'SERVICIOS CONTRATADOS', 'RETAZO DE TRUCHA AHUMADA',\n",
        "                       'FILETEO PAICHE', 'TRUCHA ENTERA']\n",
        "df_ventasFAC_limpio = df_ventasFAC_limpio[~df_ventasFAC_limpio['Cat. Producto'].isin(categorias_eliminar)]\n",
        "\n",
        "# Verificar\n",
        "print(df_ventasFAC_limpio['Cat. Producto'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qv27_G_eXIdL"
      },
      "outputs": [],
      "source": [
        "df_ventasFAC_limpio.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzdUUdQTX39x"
      },
      "outputs": [],
      "source": [
        "#Se va a verificar los estados de las facturaciones ya que nos interesan las que se han cobrado y pendientes. Pagado hace referencia a devoluciones, Anulado son correcciones de las facturas por que vamos a eliminar pagado y anulado.\n",
        "print(df_ventasFAC_limpio['Estado'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KsWzlcSZL7Y"
      },
      "outputs": [],
      "source": [
        "# Estados a eliminar\n",
        "estados_eliminar = ['Anulado', 'Pagado']\n",
        "\n",
        "# Filtrar el dataframe\n",
        "df_ventasFAC_limpio = df_ventasFAC_limpio[~df_ventasFAC_limpio['Estado'].isin(estados_eliminar)]\n",
        "\n",
        "# Verificar\n",
        "print(df_ventasFAC_limpio['Estado'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QK7f8BsRZZkX"
      },
      "outputs": [],
      "source": [
        "df_ventasFAC_limpio.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxCU6vXUaRM2"
      },
      "outputs": [],
      "source": [
        "# Convertir la columna Fecha a tipo datetime\n",
        "df_ventasFAC_limpio['Fecha'] = pd.to_datetime(df_ventasFAC_limpio['Fecha'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5oTWXXbaqqW"
      },
      "outputs": [],
      "source": [
        "df_ventasFAC_limpio = df_ventasFAC_limpio.add_prefix(\"ventas_\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFX0RChYU7-p"
      },
      "outputs": [],
      "source": [
        "df_ventasFAC_limpio.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWzkqDKwfE3h"
      },
      "source": [
        "**DEFINICIÓN DE DATA FRAME A ANALIZAR**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNH0vEJ0aSGF"
      },
      "outputs": [],
      "source": [
        "#Ahora que tenemos solo los valores que necesitamos y las categorías de interes, se va analizar.\n",
        "df_ventasFAC_limpio.describe(include='number')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QG0nzUZAbTqf"
      },
      "outputs": [],
      "source": [
        "df_ventasFAC_limpio.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlLO1bhN3Z3I"
      },
      "source": [
        "**AÑADIR FERIADOS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9C_rNtpd0W6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Diccionario completo de feriados de Ecuador 2021-2025\n",
        "feriados_ecuador = {\n",
        "    # 2021\n",
        "    '2021-01-01': 'Año Nuevo',\n",
        "    '2021-02-15': 'Carnaval',\n",
        "    '2021-02-16': 'Martes de Carnaval',\n",
        "    '2021-04-02': 'Viernes Santo',\n",
        "    '2021-04-30': 'Día del Trabajo',\n",
        "    '2021-05-24': 'Batalla de Pichincha',\n",
        "    '2021-08-09': 'Primer Grito de Independencia',\n",
        "    '2021-10-08': 'Independencia de Guayaquil',\n",
        "    '2021-11-01': 'Día de los Difuntos',\n",
        "    '2021-11-05': 'Independencia de Cuenca',\n",
        "    '2021-12-25': 'Navidad',\n",
        "\n",
        "    # 2022\n",
        "    '2021-12-31': 'Año Nuevo (traslado)',\n",
        "    '2022-01-01': 'Año Nuevo',\n",
        "    '2022-02-28': 'Carnaval',\n",
        "    '2022-03-01': 'Martes de Carnaval',\n",
        "    '2022-04-15': 'Viernes Santo',\n",
        "    '2022-05-02': 'Día del Trabajo',\n",
        "    '2022-05-23': 'Batalla de Pichincha',\n",
        "    '2022-08-12': 'Primer Grito de Independencia',\n",
        "    '2022-10-10': 'Independencia de Guayaquil',\n",
        "    '2022-11-03': 'Día de los Difuntos',\n",
        "    '2022-11-04': 'Independencia de Cuenca',\n",
        "    '2022-12-26': 'Navidad (traslado)',\n",
        "\n",
        "    # 2023\n",
        "    '2023-01-02': 'Año Nuevo (traslado)',\n",
        "    '2023-02-20': 'Carnaval',\n",
        "    '2023-02-21': 'Martes de Carnaval',\n",
        "    '2023-04-07': 'Viernes Santo',\n",
        "    '2023-05-01': 'Día del Trabajo',\n",
        "    '2023-05-26': 'Batalla de Pichincha',\n",
        "    '2023-08-11': 'Primer Grito de Independencia',\n",
        "    '2023-10-09': 'Independencia de Guayaquil',\n",
        "    '2023-11-02': 'Día de los Difuntos',\n",
        "    '2023-11-03': 'Independencia de Cuenca',\n",
        "    '2023-12-25': 'Navidad',\n",
        "\n",
        "    # 2024\n",
        "    '2024-01-01': 'Año Nuevo',\n",
        "    '2024-02-12': 'Carnaval',\n",
        "    '2024-02-13': 'Martes de Carnaval',\n",
        "    '2024-03-29': 'Viernes Santo',\n",
        "    '2024-05-03': 'Día del Trabajo',\n",
        "    '2024-05-24': 'Batalla de Pichincha',\n",
        "    '2024-08-09': 'Primer Grito de Independencia',\n",
        "    '2024-10-11': 'Independencia de Guayaquil',\n",
        "    '2024-11-01': 'Día de los Difuntos',\n",
        "    '2024-11-04': 'Independencia de Cuenca',\n",
        "    '2024-12-25': 'Navidad',\n",
        "\n",
        "    # 2025\n",
        "    '2025-01-01': 'Año Nuevo',\n",
        "    '2025-03-03': 'Carnaval',\n",
        "    '2025-03-04': 'Martes de Carnaval',\n",
        "    '2025-04-18': 'Viernes Santo',\n",
        "    '2025-05-02': 'Día del Trabajo',\n",
        "    '2025-05-23': 'Batalla de Pichincha',\n",
        "    '2025-08-11': 'Primer Grito de Independencia',\n",
        "    '2025-10-10': 'Independencia de Guayaquil',\n",
        "    '2025-11-02': 'Día de los Difuntos',\n",
        "    '2025-11-03': 'Independencia de Cuenca',\n",
        "    '2025-12-25': 'Navidad'\n",
        "}\n",
        "\n",
        "# === 1️⃣ Preparación de feriados ===\n",
        "# Convertir fechas a datetime\n",
        "feriados_ec_dict = {pd.to_datetime(fecha): nombre for fecha, nombre in feriados_ecuador.items()}\n",
        "fechas_feriados = pd.Series(list(feriados_ec_dict.keys()))\n",
        "\n",
        "# === 2️⃣ Calcular la semana (lunes como inicio de semana) ===\n",
        "df_ventasFAC_limpio['Semana'] = df_ventasFAC_limpio['ventas_Fecha'] - pd.to_timedelta(\n",
        "    df_ventasFAC_limpio['ventas_Fecha'].dt.weekday, unit='D'\n",
        ")\n",
        "\n",
        "# === 3️⃣ Determinar las semanas que contienen al menos un feriado ===\n",
        "semanas_con_feriado = (\n",
        "    fechas_feriados\n",
        "    .map(lambda d: d - pd.Timedelta(days=d.weekday()))  # obtener el lunes de esa semana\n",
        "    .drop_duplicates()\n",
        ")\n",
        "\n",
        "# === 4️⃣ Calcular semanas previas y posteriores ===\n",
        "semanas_pre = semanas_con_feriado - pd.Timedelta(weeks=1)\n",
        "semanas_post = semanas_con_feriado + pd.Timedelta(weeks=1)\n",
        "\n",
        "# === 5️⃣ Inicializar columnas ===\n",
        "df_ventasFAC_limpio['Es_Feriado'] = 0\n",
        "df_ventasFAC_limpio['Pre_Feriado'] = 0\n",
        "df_ventasFAC_limpio['Post_Feriado'] = 0\n",
        "\n",
        "# === 6️⃣ Marcar semanas ===\n",
        "# Semanas con feriado\n",
        "df_ventasFAC_limpio.loc[df_ventasFAC_limpio['Semana'].isin(semanas_con_feriado), 'Es_Feriado'] = 1\n",
        "\n",
        "# Semanas pre-feriado (si no es feriado)\n",
        "df_ventasFAC_limpio.loc[\n",
        "    (df_ventasFAC_limpio['Semana'].isin(semanas_pre)) &\n",
        "    (df_ventasFAC_limpio['Es_Feriado'] == 0),\n",
        "    'Pre_Feriado'\n",
        "] = 1\n",
        "\n",
        "# Semanas post-feriado (si no es feriado ni pre-feriado)\n",
        "df_ventasFAC_limpio.loc[\n",
        "    (df_ventasFAC_limpio['Semana'].isin(semanas_post)) &\n",
        "    (df_ventasFAC_limpio['Es_Feriado'] == 0) &\n",
        "    (df_ventasFAC_limpio['Pre_Feriado'] == 0),\n",
        "    'Post_Feriado'\n",
        "] = 1\n",
        "\n",
        "# === 7️⃣ Nombre del feriado (solo para el día exacto, opcional) ===\n",
        "df_ventasFAC_limpio['Nombre_Feriado'] = df_ventasFAC_limpio['ventas_Fecha'].map(feriados_ec_dict).fillna('')\n",
        "\n",
        "# === 8️⃣ Reporte de validación ===\n",
        "print(\"=== RESUMEN DE FERIADOS (POR SEMANA) ===\")\n",
        "print(f\"Total de feriados únicos definidos: {len(feriados_ecuador)}\")\n",
        "print(f\"  - Semanas con feriado: {df_ventasFAC_limpio['Es_Feriado'].sum()}\")\n",
        "print(f\"  - Semanas pre-feriado: {df_ventasFAC_limpio['Pre_Feriado'].sum()}\")\n",
        "print(f\"  - Semanas post-feriado: {df_ventasFAC_limpio['Post_Feriado'].sum()}\")\n",
        "print(f\"  - Semanas normales: {((df_ventasFAC_limpio['Es_Feriado'] == 0) & (df_ventasFAC_limpio['Pre_Feriado'] == 0) & (df_ventasFAC_limpio['Post_Feriado'] == 0)).sum()}\")\n",
        "\n",
        "# === 9️⃣ Ejemplos de cada tipo ===\n",
        "print(\"\\n=== SEMANAS CON FERIADO ===\")\n",
        "feriados_en_datos = df_ventasFAC_limpio[df_ventasFAC_limpio['Es_Feriado'] == 1][['Semana']].drop_duplicates().sort_values('Semana')\n",
        "print(feriados_en_datos.to_string(index=False))\n",
        "\n",
        "print(\"\\n=== SEMANAS PRE-FERIADO ===\")\n",
        "ejemplos_pre = df_ventasFAC_limpio[df_ventasFAC_limpio['Pre_Feriado'] == 1][['ventas_Fecha', 'Semana']].drop_duplicates('Semana').sort_values('Semana').head(10)\n",
        "print(ejemplos_pre.to_string(index=False))\n",
        "\n",
        "print(\"\\n=== SEMANAS POST-FERIADO ===\")\n",
        "ejemplos_post = df_ventasFAC_limpio[df_ventasFAC_limpio['Post_Feriado'] == 1][['ventas_Fecha', 'Semana']].drop_duplicates('Semana').sort_values('Semana').head(10)\n",
        "print(ejemplos_post.to_string(index=False))\n",
        "\n",
        "print(\"\\n=== ESTRUCTURA FINAL DEL DATAFRAME ===\")\n",
        "print(df_ventasFAC_limpio.info())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpEpCPah4KOz"
      },
      "source": [
        "**GENERAR INVENTARIO DE LA BASE DE DATOS DE PRODUCCION Y AÑADIR A LA BASE DE DATOS DE VENTAS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuNXXqeIGn9s"
      },
      "outputs": [],
      "source": [
        "df_ventasFAC_limpio.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAqhYg6IHUXC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Extraer df_produccion con Año_Semana\n",
        "df_produccion['Año'] = df_produccion['produccion_Fecha'].dt.isocalendar().year\n",
        "df_produccion['Semana'] = df_produccion['produccion_Fecha'].dt.isocalendar().week\n",
        "df_produccion['Año_Semana'] = df_produccion['Año'].astype(str) + '-' + df_produccion['Semana'].astype(str).str.zfill(2)\n",
        "\n",
        "# Añadir las mismas a ventas (ya tienes estas columnas, pero por si acaso)\n",
        "df_ventasFAC_limpio['ventas_Año'] = df_ventasFAC_limpio['ventas_Fecha'].dt.year\n",
        "df_ventasFAC_limpio['ventas_Semana'] = df_ventasFAC_limpio['ventas_Fecha'].dt.isocalendar().week\n",
        "df_ventasFAC_limpio['Año_Semana'] = df_ventasFAC_limpio['ventas_Año'].astype(str) + '-' + df_ventasFAC_limpio['ventas_Semana'].astype(str).str.zfill(2)\n",
        "\n",
        "# FILTRAR: Solo 2021-2025\n",
        "df_produccion_filtrado = df_produccion[df_produccion['Año'].between(2021, 2025)].copy()\n",
        "df_ventasFAC_filtrado = df_ventasFAC_limpio[df_ventasFAC_limpio['ventas_Año'].between(2021, 2025)].copy()\n",
        "\n",
        "# Agregar producción por semana y categoría\n",
        "produccion_semanal = df_produccion_filtrado.groupby(['Año_Semana', 'produccion_Cat. Producto']).agg({\n",
        "    'produccion_Cantidad': 'sum',\n",
        "    'produccion_Fecha': 'min'\n",
        "}).reset_index()\n",
        "produccion_semanal.columns = ['Año_Semana', 'Categoria', 'Cantidad_Producida', 'Fecha']\n",
        "\n",
        "# Agregar ventas por semana y categoría (INCLUYENDO VARIABLES DE FERIADOS)\n",
        "ventas_semanal = df_ventasFAC_filtrado.groupby(['Año_Semana', 'ventas_Cat. Producto']).agg({\n",
        "    'ventas_Cantidad': 'sum',\n",
        "    'ventas_Precio': 'mean',\n",
        "    'ventas_Total': 'sum',\n",
        "    'Es_Feriado': 'max',           # Si hay al menos 1 día feriado en la semana\n",
        "    'Pre_Feriado': 'max',          # Si hay al menos 1 día pre-feriado\n",
        "    'Post_Feriado': 'max',         # Si hay al menos 1 día post-feriado\n",
        "    'ventas_Fecha': 'min'\n",
        "}).reset_index()\n",
        "\n",
        "ventas_semanal.columns = ['Año_Semana', 'Categoria', 'Cantidad_Vendida', 'Precio_Promedio',\n",
        "                          'Ventas_Total', 'Es_Feriado', 'Pre_Feriado', 'Post_Feriado', 'Fecha_Venta']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQP4xVD0HrPM"
      },
      "outputs": [],
      "source": [
        "# Crear df_analisis con la unión de los dos data frames\n",
        "df_analisis = pd.merge(\n",
        "    produccion_semanal,\n",
        "    ventas_semanal,\n",
        "    on=['Año_Semana', 'Categoria'],\n",
        "    how='outer'\n",
        ")\n",
        "\n",
        "# Rellenar NaN con 0 para cantidades\n",
        "df_analisis['Cantidad_Producida'] = df_analisis['Cantidad_Producida'].fillna(0)\n",
        "df_analisis['Cantidad_Vendida'] = df_analisis['Cantidad_Vendida'].fillna(0)\n",
        "df_analisis['Ventas_Total'] = df_analisis['Ventas_Total'].fillna(0)\n",
        "df_analisis['Es_Feriado'] = df_analisis['Es_Feriado'].fillna(0).astype(int)\n",
        "df_analisis['Pre_Feriado'] = df_analisis['Pre_Feriado'].fillna(0).astype(int)\n",
        "df_analisis['Post_Feriado'] = df_analisis['Post_Feriado'].fillna(0).astype(int)\n",
        "\n",
        "# Usar Fecha de producción o venta (la que exista)\n",
        "df_analisis['Fecha'] = df_analisis['Fecha'].fillna(df_analisis['Fecha_Venta'])\n",
        "df_analisis['Fecha'] = pd.to_datetime(df_analisis['Fecha'])\n",
        "\n",
        "# Extraer Año y Semana_Num\n",
        "df_analisis['Año'] = df_analisis['Fecha'].dt.year\n",
        "df_analisis['Semana_Num'] = df_analisis['Fecha'].dt.isocalendar().week\n",
        "\n",
        "# Ordenar por categoría, año y semana\n",
        "df_analisis = df_analisis.sort_values(['Categoria', 'Año', 'Semana_Num']).reset_index(drop=True)\n",
        "\n",
        "# Rellenar precios promedios\n",
        "df_analisis['Precio_Promedio'] = df_analisis.groupby(['Año', 'Categoria'])['Precio_Promedio'].ffill()\n",
        "df_analisis['Precio_Promedio'] = df_analisis.groupby(['Año', 'Categoria'])['Precio_Promedio'].bfill()\n",
        "precio_promedio_anual = df_analisis.groupby(['Año', 'Categoria'])['Precio_Promedio'].transform('mean')\n",
        "df_analisis['Precio_Promedio'] = df_analisis['Precio_Promedio'].fillna(precio_promedio_anual)\n",
        "\n",
        "print(\"\\n VERIFICACIÓN DE PRECIOS\")\n",
        "print(f\"Precios nulos restantes: {df_analisis['Precio_Promedio'].isnull().sum()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86IPGPk-MviI"
      },
      "outputs": [],
      "source": [
        "df_analisis.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3A0NXJrHzjg"
      },
      "outputs": [],
      "source": [
        "# INVENTARIO INICIAL AJUSTADO CON MERMAS\n",
        "inventario_inicial = {\n",
        "    'FILETE CON CORTE C': 651.63,\n",
        "    'FILETE DE TRUCHA AHUMADA': 172.03,\n",
        "    'PORCIONADO FILETE CORTE C': 57.68,\n",
        "}\n",
        "\n",
        "mermas_por_categoria = {\n",
        "    'FILETE CON CORTE C': 0.03,  # 3%\n",
        "    'FILETE DE TRUCHA AHUMADA': 0.02,  # 2%\n",
        "    'PORCIONADO FILETE CORTE C': 0.05,  # 5%\n",
        "}\n",
        "\n",
        "# Calcular mermas por congelación/descongelación\n",
        "df_analisis['Mermas'] = df_analisis.apply(\n",
        "    lambda row: row['Cantidad_Producida'] * mermas_por_categoria.get(row['Categoria'], 0.03),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# CALCULAR INVENTARIO (acumulativo por categoría con inventario inicial y mermas)\n",
        "df_analisis['Inventario'] = 0.0\n",
        "\n",
        "for categoria in df_analisis['Categoria'].unique():\n",
        "    mask = df_analisis['Categoria'] == categoria\n",
        "    inv_inicial = inventario_inicial.get(categoria, 0.0)\n",
        "\n",
        "    # Calcular diferencia (Producción - Mermas - Ventas)\n",
        "    diferencia = (\n",
        "        df_analisis.loc[mask, 'Cantidad_Producida'] -\n",
        "        df_analisis.loc[mask, 'Mermas'] -\n",
        "        df_analisis.loc[mask, 'Cantidad_Vendida']\n",
        "    )\n",
        "\n",
        "    df_analisis.loc[mask, 'Inventario'] = inv_inicial + diferencia.cumsum()\n",
        "\n",
        "# Limpiar columnas auxiliares\n",
        "df_analisis = df_analisis.drop(columns=['Fecha_Venta'])\n",
        "\n",
        "# Reordenar columnas (INCLUYENDO VARIABLES DE FERIADOS)\n",
        "df_analisis = df_analisis[[\n",
        "    'Año_Semana', 'Año', 'Semana_Num', 'Categoria', 'Fecha',\n",
        "    'Es_Feriado', 'Pre_Feriado', 'Post_Feriado',\n",
        "    'Cantidad_Producida', 'Mermas', 'Cantidad_Vendida', 'Inventario',\n",
        "    'Precio_Promedio', 'Ventas_Total'\n",
        "]]\n",
        "\n",
        "# Mostrar información del nuevo DataFrame\n",
        "print(\" DF_ANALISIS CREADO (2021-2025 CON INVENTARIO, MERMAS Y VARIABLES DE FERIADOS)\")\n",
        "print(df_analisis.info())\n",
        "\n",
        "print(\"\\n RANGO DE AÑOS\")\n",
        "print(f\"Años únicos: {sorted(df_analisis['Año'].unique())}\")\n",
        "print(f\"Rango de fechas: {df_analisis['Fecha'].min()} a {df_analisis['Fecha'].max()}\")\n",
        "\n",
        "print(\"\\nINVENTARIO INICIAL APLICADO\")\n",
        "for cat, inv in inventario_inicial.items():\n",
        "    primera_semana = df_analisis[df_analisis['Categoria'] == cat].iloc[0] if cat in df_analisis['Categoria'].values else None\n",
        "    if primera_semana is not None:\n",
        "        print(f\"{cat}: {inv} kg\")\n",
        "        print(f\"  → Primera semana ({primera_semana['Año_Semana']}): Inventario = {primera_semana['Inventario']:.2f} kg\")\n",
        "\n",
        "print(\"\\n RESUMEN DE MERMAS POR CATEGORÍA\")\n",
        "print(df_analisis.groupby('Categoria')['Mermas'].agg(['sum', 'mean']).round(2))\n",
        "\n",
        "print(\"\\n ESTADÍSTICAS DE INVENTARIO POR CATEGORÍA\")\n",
        "print(df_analisis.groupby('Categoria')['Inventario'].describe().round(2))\n",
        "\n",
        "print(\"\\n VERIFICACIÓN - Inventario negativo\")\n",
        "inventario_negativo = df_analisis[df_analisis['Inventario'] < 0]\n",
        "if len(inventario_negativo) > 0:\n",
        "    print(f\" {len(inventario_negativo)} semanas con inventario negativo\")\n",
        "    print(inventario_negativo[['Año_Semana', 'Categoria', 'Inventario']].head(10))\n",
        "else:\n",
        "    print(\" No hay inventarios negativos\")\n",
        "\n",
        "print(\"\\nVERIFICACIÓN - Variables de Feriados\")\n",
        "print(f\"Semanas con feriados: {df_analisis['Es_Feriado'].sum()}\")\n",
        "print(f\"Semanas pre-feriado: {df_analisis['Pre_Feriado'].sum()}\")\n",
        "print(f\"Semanas post-feriado: {df_analisis['Post_Feriado'].sum()}\")\n",
        "\n",
        "# Ver algunas semanas con feriados\n",
        "print(\"\\n EJEMPLOS DE SEMANAS CON FERIADOS:\")\n",
        "ejemplos_feriados = df_analisis[df_analisis['Es_Feriado'] == 1][['Año_Semana', 'Categoria', 'Es_Feriado', 'Cantidad_Vendida']].head(10)\n",
        "print(ejemplos_feriados.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIuq1iFU50MD"
      },
      "outputs": [],
      "source": [
        "df_analisis.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kr5yRDFyEzqk"
      },
      "outputs": [],
      "source": [
        "# Descargar ambos formatos\n",
        "df_analisis.to_excel('df_analisis_completo.xlsx', index=False)\n",
        "df_analisis.to_csv('df_analisis_completo.csv', index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"Archivos descargados:\")\n",
        "print(\"   - df_analisis_completo.xlsx\")\n",
        "print(\"   - df_analisis_completo.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yN-YN8ltYiZ"
      },
      "outputs": [],
      "source": [
        "df_analisis.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33oyO-nIoBFR"
      },
      "outputs": [],
      "source": [
        "# CONFIGURACIÓN GLOBAL CON PALETA ROCKET\n",
        "sns.set_theme(style=\"whitegrid\", palette=\"rocket\")\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "# Obtener colores de la paleta rocket\n",
        "colores_rocket = sns.color_palette(\"rocket\", n_colors=3)\n",
        "\n",
        "# Variables seleccionadas estratégicamente\n",
        "variables_boxplot = [\n",
        "    'Cantidad_Vendida',\n",
        "]\n",
        "\n",
        "# Títulos personalizados\n",
        "titulos = [\n",
        "    'Cantidad Vendida\\n(Variable Dependiente)',\n",
        "]\n",
        "\n",
        "# Crear figura con subplots\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "fig.suptitle('Análisis de Dispersión por Categoría de Producto (Variables Clave del Modelo)',\n",
        "             fontsize=16, fontweight='bold', y=0.98)\n",
        "\n",
        "# Crear el boxplot\n",
        "variable = variables_boxplot[0]\n",
        "titulo = titulos[0]\n",
        "\n",
        "# Boxplot con paleta rocket\n",
        "sns.boxplot(data=df_analisis,\n",
        "            x='Categoria',\n",
        "            y=variable,\n",
        "            ax=ax,\n",
        "            palette='rocket',\n",
        "            linewidth=1.5,\n",
        "            flierprops=dict(marker='o', markerfacecolor='red',\n",
        "                           markersize=5, alpha=0.5))\n",
        "\n",
        "# Título del subplot\n",
        "ax.set_title(titulo, fontsize=14, fontweight='bold', pad=15)\n",
        "ax.set_xlabel('Categoría', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Valor (kg)', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Etiquetas del eje X en horizontal con dos líneas\n",
        "categorias_labels = []\n",
        "for cat in df_analisis['Categoria'].unique():\n",
        "    # Dividir el nombre en dos líneas si es largo\n",
        "    palabras = cat.split()\n",
        "    if len(palabras) > 2:\n",
        "        # Dividir aproximadamente por la mitad\n",
        "        mitad = len(palabras) // 2\n",
        "        linea1 = ' '.join(palabras[:mitad])\n",
        "        linea2 = ' '.join(palabras[mitad:])\n",
        "        categorias_labels.append(f'{linea1}\\n{linea2}')\n",
        "    else:\n",
        "        categorias_labels.append(cat)\n",
        "\n",
        "ax.set_xticklabels(categorias_labels, rotation=0, ha='center', fontsize=10)\n",
        "ax.tick_params(axis='y', labelsize=10)\n",
        "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "plt.tight_layout()  # Agregado para mejor espaciado\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkN1jaDOnz8S"
      },
      "source": [
        "Se agregan las categorias por semana debido a que el interés es en la cantidad de materia prima a comprar. Se requieren más datos para predecir las ventas por categorías y por el crecimiento grande existe mucha variabilidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hehrx-R-nzbg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-raxn1qYfyUY"
      },
      "outputs": [],
      "source": [
        "df_total = df_analisis.groupby(['Año_Semana', 'Año', 'Semana_Num']).agg({\n",
        "    'Fecha': 'first',                 # Tomar la primera fecha de la semana\n",
        "    'Cantidad_Producida': 'sum',      # Sumar producción de las 3 categorías\n",
        "    'Mermas': 'sum',                  # Sumar mermas\n",
        "    'Cantidad_Vendida': 'sum',        # Sumar ventas de las 3 categorías\n",
        "    'Inventario': 'sum',              # Sumar inventario\n",
        "    'Precio_Promedio': 'mean',        # Promedio de precios\n",
        "    'Ventas_Total': 'sum',            # Sumar ventas totales en $\n",
        "    'Es_Feriado': 'max',              # Mantener si hay feriado\n",
        "    'Pre_Feriado': 'max',             # Mantener pre-feriado\n",
        "    'Post_Feriado': 'max'             # Mantener post-feriado\n",
        "}).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlNsepnGbPm9"
      },
      "outputs": [],
      "source": [
        "df_total.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zkg16J5_7bo"
      },
      "outputs": [],
      "source": [
        "# Guardar como CSV\n",
        "df_total.to_csv('df_total.csv', index=False, encoding='utf-8')\n",
        "print(\"✓ Archivo guardado: df_total.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuszrkS7AFcB"
      },
      "outputs": [],
      "source": [
        "# Guardar como Excel\n",
        "df_total.to_excel('df_total.xlsx', index=False, engine='openpyxl')\n",
        "print(\"✓ Archivo guardado: df_total.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0DynMErhqyI"
      },
      "source": [
        "**MEDIDAS DE DISPERSIÓN Y VARIABILIDAD**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jggZbIAR0E1Y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def identificar_outliers(df, columna):\n",
        "    \"\"\"Identifica outliers usando el método IQR\"\"\"\n",
        "    Q1 = df[columna].quantile(0.25)\n",
        "    Q3 = df[columna].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    limite_inferior = Q1 - 1.5 * IQR\n",
        "    limite_superior = Q3 + 1.5 * IQR\n",
        "\n",
        "    outliers = df[(df[columna] < limite_inferior) | (df[columna] > limite_superior)]\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"IDENTIFICACIÓN DE OUTLIERS - {columna}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Q1 (25%): {Q1:.2f}\")\n",
        "    print(f\"Q3 (75%): {Q3:.2f}\")\n",
        "    print(f\"IQR: {IQR:.2f}\")\n",
        "    print(f\"Límite Inferior: {limite_inferior:.2f}\")\n",
        "    print(f\"Límite Superior: {limite_superior:.2f}\")\n",
        "    print(f\"\\nOutliers detectados: {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)\")\n",
        "    print(f\"Valores normales: {len(df) - len(outliers)} ({(len(df)-len(outliers))/len(df)*100:.2f}%)\")\n",
        "\n",
        "    return outliers, limite_inferior, limite_superior\n",
        "\n",
        "# Identificar outliers en Cantidad_Vendida\n",
        "outliers, lim_inf, lim_sup = identificar_outliers(df_total, 'Cantidad_Vendida')\n",
        "\n",
        "# Visualizar\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Boxplot\n",
        "axes[0].boxplot(df_total['Cantidad_Vendida'])\n",
        "axes[0].set_title('Boxplot - Cantidad Vendida\\n(Con Outliers) data frame agregado', fontweight='bold')\n",
        "axes[0].set_ylabel('Cantidad Vendida (kg)')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Serie temporal con outliers marcados\n",
        "axes[1].plot(df_total.index, df_total['Cantidad_Vendida'],\n",
        "             linewidth=1, color='blue', alpha=0.6, label='Datos originales')\n",
        "axes[1].scatter(outliers.index, outliers['Cantidad_Vendida'],\n",
        "                color='red', s=50, zorder=5, label=f'Outliers ({len(outliers)})')\n",
        "axes[1].axhline(y=lim_sup, color='red', linestyle='--', alpha=0.5, label='Límites IQR')\n",
        "axes[1].axhline(y=lim_inf, color='red', linestyle='--', alpha=0.5)\n",
        "axes[1].set_title('Serie Temporal - Outliers Identificados', fontweight='bold')\n",
        "axes[1].set_xlabel('Observaciones')\n",
        "axes[1].set_ylabel('Cantidad Vendida (kg)')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwpbVP7fsKtx"
      },
      "outputs": [],
      "source": [
        "df_total.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0jdlz70efqo"
      },
      "source": [
        "**GRAFICO DE BARRAS**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZkFUem1zfOF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# CONFIGURACIÓN GLOBAL CON PALETA ROCKET\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\", palette=\"rocket\")\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "# Obtener colores de la paleta rocket\n",
        "colores_rocket = sns.color_palette(\"rocket\", n_colors=10)\n",
        "\n",
        "\n",
        "\n",
        "# GRÁFICO 3: BARRAS AGRUPADAS (CANTIDAD VENDIDA VS PRODUCIDA)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# Datos\n",
        "categorias = df_analisis['Categoria'].unique()\n",
        "x_pos = np.arange(len(categorias))\n",
        "width = 0.35\n",
        "\n",
        "vendida = [df_analisis[df_analisis['Categoria']==cat]['Cantidad_Vendida'].mean()\n",
        "           for cat in categorias]\n",
        "producida = [df_analisis[df_analisis['Categoria']==cat]['Cantidad_Producida'].mean()\n",
        "             for cat in categorias]\n",
        "\n",
        "# Barras agrupadas\n",
        "bars1 = ax.bar(x_pos - width/2, vendida, width,\n",
        "               label='Cantidad Vendida',\n",
        "               color=colores_rocket[2], alpha=0.8,\n",
        "               edgecolor='black', linewidth=1.2)\n",
        "\n",
        "bars2 = ax.bar(x_pos + width/2, producida, width,\n",
        "               label='Cantidad Producida',\n",
        "               color=colores_rocket[6], alpha=0.8,\n",
        "               edgecolor='black', linewidth=1.2)\n",
        "\n",
        "# Valores encima\n",
        "for i, v in enumerate(vendida):\n",
        "    ax.text(i - width/2, v, f'{v:.1f}',\n",
        "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "for i, v in enumerate(producida):\n",
        "    ax.text(i + width/2, v, f'{v:.1f}',\n",
        "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Configuración\n",
        "ax.set_title('Comparación: Cantidad Vendida vs Cantidad Producida por Categoría',\n",
        "             fontsize=14, fontweight='bold', pad=15)\n",
        "ax.set_xlabel('Categoría de Producto', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Cantidad Promedio (kg)', fontsize=12, fontweight='bold')\n",
        "ax.set_xticks(x_pos)\n",
        "\n",
        "# Etiquetas en dos líneas\n",
        "etiquetas = []\n",
        "for cat in categorias:\n",
        "    palabras = cat.split()\n",
        "    if len(palabras) > 2:\n",
        "        mitad = len(palabras) // 2\n",
        "        etiquetas.append(' '.join(palabras[:mitad]) + '\\n' + ' '.join(palabras[mitad:]))\n",
        "    else:\n",
        "        etiquetas.append(cat)\n",
        "\n",
        "ax.set_xticklabels(etiquetas, fontsize=10)\n",
        "ax.legend(fontsize=11, loc='upper right', framealpha=0.95)\n",
        "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('barras_vendida_vs_producida.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6SJrXteptKB"
      },
      "source": [
        "**WINSORIZACION**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjEmtDSy0sL1"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import mstats\n",
        "\n",
        "def winsorizar(df, columna, limits=(0.05, 0.05)):\n",
        "    \"\"\"\n",
        "    Reemplaza outliers extremos con percentiles\n",
        "    limits: (percentil_inferior, percentil_superior) a recortar\n",
        "    \"\"\"\n",
        "    df_tratado = df.copy()\n",
        "\n",
        "    # Calcular CV antes\n",
        "    cv_antes = (df[columna].std() / df[columna].mean()) * 100\n",
        "\n",
        "    # Aplicar winsorización\n",
        "    df_tratado[columna] = mstats.winsorize(df[columna], limits=limits)\n",
        "\n",
        "    # Calcular CV después\n",
        "    cv_despues = (df_tratado[columna].std() / df_tratado[columna].mean()) * 100\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"WINSORIZACIÓN - {columna}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Límites aplicados: {limits[0]*100}% inferior, {limits[1]*100}% superior\")\n",
        "    print(f\"\\nCV Antes: {cv_antes:.2f}%\")\n",
        "    print(f\"CV Después: {cv_despues:.2f}%\")\n",
        "    print(f\"Reducción: {cv_antes - cv_despues:.2f} puntos porcentuales ({((cv_antes-cv_despues)/cv_antes*100):.1f}%)\")\n",
        "    print(f\"\\nRegistros modificados: {(df[columna] != df_tratado[columna]).sum()}\")\n",
        "\n",
        "    return df_tratado, cv_antes, cv_despues\n",
        "\n",
        "# Aplicar winsorización\n",
        "df_winsorizado, cv_antes, cv_despues = winsorizar(df_total, 'Cantidad_Vendida', limits=(0.05, 0.05))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qU-DbDljvWvr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Función para calcular el coeficiente de variación\n",
        "def coeficiente_variacion(data):\n",
        "    \"\"\"Calcula el coeficiente de variación (CV = desviación estándar / media * 100)\"\"\"\n",
        "    return (data.std() / data.mean()) * 100\n",
        "\n",
        "# 1. Coeficiente de Variación por Categoría (df_analisis)\n",
        "print(\"COEFICIENTE DE VARIACIÓN POR CATEGORÍA (df_analisis)\")\n",
        "\n",
        "cv_categorias = {}\n",
        "for categoria in df_analisis['Categoria'].unique():\n",
        "    datos_cat = df_analisis[df_analisis['Categoria'] == categoria]['Cantidad_Vendida']\n",
        "    cv = coeficiente_variacion(datos_cat)\n",
        "    cv_categorias[categoria] = cv\n",
        "\n",
        "    print(f\"\\n{categoria}:\")\n",
        "    print(f\"  Media: {datos_cat.mean():.2f} kg\")\n",
        "    print(f\"  Desv. Estándar: {datos_cat.std():.2f} kg\")\n",
        "    print(f\"  Coeficiente de Variación: {cv:.2f}%\")\n",
        "\n",
        "# 2. Coeficiente de Variación Total (df_total)\n",
        "print(\"COEFICIENTE DE VARIACIÓN TOTAL (df_total)\")\n",
        "\n",
        "cv_total = coeficiente_variacion(df_total['Cantidad_Vendida'])\n",
        "print(f\"\\n  Media: {df_total['Cantidad_Vendida'].mean():.2f} kg\")\n",
        "print(f\"  Desv. Estándar: {df_total['Cantidad_Vendida'].std():.2f} kg\")\n",
        "print(f\"  Coeficiente de Variación: {cv_total:.2f}%\")\n",
        "\n",
        "# 3. Comparación y Análisis\n",
        "print(\"ANÁLISIS COMPARATIVO\")\n",
        "\n",
        "cv_promedio_categorias = np.mean(list(cv_categorias.values()))\n",
        "print(f\"\\nCV Promedio de Categorías: {cv_promedio_categorias:.2f}%\")\n",
        "print(f\"CV Total (Unificado): {cv_total:.2f}%\")\n",
        "print(f\"\\nReducción de Variabilidad: {cv_promedio_categorias - cv_total:.2f} puntos porcentuales\")\n",
        "print(f\"Mejora Relativa: {((cv_promedio_categorias - cv_total) / cv_promedio_categorias * 100):.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhNZbY9mU7Qb"
      },
      "outputs": [],
      "source": [
        "# Boxplot comparativo: df_total vs df_winsorizado en los mismos ejes\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# Preparar datos para ambos boxplots\n",
        "data = [df_total['Cantidad_Vendida'], df_winsorizado['Cantidad_Vendida']]\n",
        "labels = ['Originales', 'Tratados']\n",
        "\n",
        "# Crear boxplots\n",
        "bp = ax.boxplot(data,\n",
        "                labels=labels,\n",
        "                vert=True,\n",
        "                patch_artist=True,\n",
        "                widths=0.6,\n",
        "                medianprops=dict(color='red', linewidth=2.5),\n",
        "                whiskerprops=dict(color='black', linewidth=1.5),\n",
        "                capprops=dict(color='black', linewidth=1.5),\n",
        "                flierprops=dict(marker='o', markerfacecolor='red', markersize=6, alpha=0.5))\n",
        "\n",
        "# Colorear cada boxplot\n",
        "colors = [sns.color_palette(\"rocket\")[2], sns.color_palette(\"rocket\")[3]]\n",
        "for patch, color in zip(bp['boxes'], colors):\n",
        "    patch.set_facecolor(color)\n",
        "    patch.set_alpha(0.7)\n",
        "\n",
        "ax.set_title('Comparación de Distribuciones: Originales vs Tratados',\n",
        "             fontsize=16, fontweight='bold', pad=20)\n",
        "ax.set_ylabel('Cantidad Vendida (kg)', fontsize=13, fontweight='bold')\n",
        "ax.set_xlabel('Dataset', fontsize=13, fontweight='bold')\n",
        "\n",
        "# === ESTADÍSTICAS df_total (posición 1) ===\n",
        "media1 = df_total['Cantidad_Vendida'].mean()\n",
        "mediana1 = df_total['Cantidad_Vendida'].median()\n",
        "q1_1 = df_total['Cantidad_Vendida'].quantile(0.25)\n",
        "q3_1 = df_total['Cantidad_Vendida'].quantile(0.75)\n",
        "iqr1 = q3_1 - q1_1\n",
        "whisker_low1 = bp['whiskers'][0].get_ydata()[1]\n",
        "whisker_high1 = bp['whiskers'][1].get_ydata()[1]\n",
        "\n",
        "# Textos lado izquierdo (df_total)\n",
        "ax.text(0.65, q3_1, f'Q3: {q3_1:.2f}', fontsize=9, ha='left', va='center',\n",
        "        bbox=dict(boxstyle='round,pad=0.4', facecolor='lightgray', alpha=0.8))\n",
        "ax.text(0.65, mediana1, f'Med: {mediana1:.2f}', fontsize=9, ha='left', va='center',\n",
        "        bbox=dict(boxstyle='round,pad=0.4', facecolor='lightgray', alpha=0.8))\n",
        "ax.text(0.65, q1_1, f'Q1: {q1_1:.2f}', fontsize=9, ha='left', va='center',\n",
        "        bbox=dict(boxstyle='round,pad=0.4', facecolor='lightgray', alpha=0.8))\n",
        "\n",
        "# Textos lado derecho (df_total)\n",
        "ax.text(1.35, media1, f'μ: {media1:.2f}', fontsize=9, ha='right', va='center',\n",
        "        bbox=dict(boxstyle='round,pad=0.4', facecolor='lightgray', alpha=0.8))\n",
        "ax.text(1.35, whisker_high1, f'Máx: {whisker_high1:.2f}', fontsize=9, ha='right', va='center',\n",
        "        bbox=dict(boxstyle='round,pad=0.4', facecolor='lightgray', alpha=0.8))\n",
        "ax.text(1.35, whisker_low1, f'Mín: {whisker_low1:.2f}', fontsize=9, ha='right', va='center',\n",
        "        bbox=dict(boxstyle='round,pad=0.4', facecolor='lightgray', alpha=0.8))\n",
        "\n",
        "# IQR arriba (df_total)\n",
        "ax.text(1, ax.get_ylim()[1] * 0.98, f'IQR: {iqr1:.2f}', fontsize=9, ha='center', va='top',\n",
        "        fontweight='bold', bbox=dict(boxstyle='round,pad=0.4', facecolor='white', edgecolor='black'))\n",
        "\n",
        "# === ESTADÍSTICAS df_winsorizado (posición 2) ===\n",
        "media2 = df_winsorizado['Cantidad_Vendida'].mean()\n",
        "mediana2 = df_winsorizado['Cantidad_Vendida'].median()\n",
        "q1_2 = df_winsorizado['Cantidad_Vendida'].quantile(0.25)\n",
        "q3_2 = df_winsorizado['Cantidad_Vendida'].quantile(0.75)\n",
        "iqr2 = q3_2 - q1_2\n",
        "whisker_low2 = bp['whiskers'][2].get_ydata()[1]\n",
        "whisker_high2 = bp['whiskers'][3].get_ydata()[1]\n",
        "\n",
        "# Textos lado izquierdo (df_winsorizado)\n",
        "ax.text(1.65, q3_2, f'Q3: {q3_2:.2f}', fontsize=9, ha='left', va='center',\n",
        "        bbox=dict(boxstyle='round,pad=0.4', facecolor='lightgray', alpha=0.8))\n",
        "ax.text(1.65, mediana2, f'Med: {mediana2:.2f}', fontsize=9, ha='left', va='center',\n",
        "        bbox=dict(boxstyle='round,pad=0.4', facecolor='lightgray', alpha=0.8))\n",
        "ax.text(1.65, q1_2, f'Q1: {q1_2:.2f}', fontsize=9, ha='left', va='center',\n",
        "        bbox=dict(boxstyle='round,pad=0.4', facecolor='lightgray', alpha=0.8))\n",
        "\n",
        "# Textos lado derecho (df_winsorizado)\n",
        "ax.text(2.35, media2, f'μ: {media2:.2f}', fontsize=9, ha='right', va='center',\n",
        "        bbox=dict(boxstyle='round,pad=0.4', facecolor='lightgray', alpha=0.8))\n",
        "ax.text(2.35, whisker_high2, f'Máx: {whisker_high2:.2f}', fontsize=9, ha='right', va='center',\n",
        "        bbox=dict(boxstyle='round,pad=0.4', facecolor='lightgray', alpha=0.8))\n",
        "ax.text(2.35, whisker_low2, f'Mín: {whisker_low2:.2f}', fontsize=9, ha='right', va='center',\n",
        "        bbox=dict(boxstyle='round,pad=0.4', facecolor='lightgray', alpha=0.8))\n",
        "\n",
        "# IQR arriba (df_winsorizado)\n",
        "ax.text(2, ax.get_ylim()[1] * 0.98, f'IQR: {iqr2:.2f}', fontsize=9, ha='center', va='top',\n",
        "        fontweight='bold', bbox=dict(boxstyle='round,pad=0.4', facecolor='white', edgecolor='black'))\n",
        "\n",
        "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "plt.tight_layout()\n",
        "plt.savefig('boxplot_comparacion.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g4YuufeeyiV"
      },
      "source": [
        "**CRECIMIENTO DE VENTAS**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4-ktIYVcfL4"
      },
      "outputs": [],
      "source": [
        "df_winsorizado.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDKnsyiP662c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Asegurarte de tener la columna Semana_Num\n",
        "if 'Semana_Num' not in df_winsorizado.columns:\n",
        "    df_winsorizado['Semana_Num'] = df_winsorizado['Fecha'].dt.isocalendar().week\n",
        "\n",
        "# Agrupar por semana del año (promedio de todos los años)\n",
        "patron_semanal = df_winsorizado.groupby('Semana_Num')['Cantidad_Vendida'].agg([\n",
        "    ('Media', 'mean'),\n",
        "    ('Desv_Std', 'std'),\n",
        "    ('Min', 'min'),\n",
        "    ('Max', 'max')\n",
        "]).reset_index()\n",
        "\n",
        "# Crear el gráfico\n",
        "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
        "fig.suptitle('Patrón Estacional de Cantidad Vendida por Semana del Año\\n(Promedio 2022-2025)',\n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "\n",
        "# GRÁFICO 1: Línea con banda de confianza\n",
        "axes[0].plot(patron_semanal['Semana_Num'], patron_semanal['Media'],\n",
        "             linewidth=2.5, color='#e74c3c', marker='o', markersize=4, label='Media')\n",
        "\n",
        "# Banda de confianza (±1 desviación estándar)\n",
        "axes[0].fill_between(patron_semanal['Semana_Num'],\n",
        "                      patron_semanal['Media'] - patron_semanal['Desv_Std'],\n",
        "                      patron_semanal['Media'] + patron_semanal['Desv_Std'],\n",
        "                      alpha=0.3, color='#e74c3c', label='±1 Desv. Std')\n",
        "\n",
        "axes[0].set_xlabel('Semana del Año', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Cantidad Vendida (kg)', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('Promedio y Variabilidad por Semana', fontsize=13, pad=10)\n",
        "axes[0].set_xticks(range(1, 54, 2))\n",
        "axes[0].grid(True, alpha=0.3, linestyle='--')\n",
        "axes[0].legend(fontsize=11, loc='best')\n",
        "\n",
        "# GRÁFICO 2: Boxplot por semana\n",
        "# Crear datos para boxplot (todas las observaciones por semana)\n",
        "datos_boxplot = []\n",
        "semanas_boxplot = []\n",
        "for semana in range(1, 53):\n",
        "    datos_semana = df_winsorizado[df_winsorizado['Semana_Num'] == semana]['Cantidad_Vendida'].values\n",
        "    if len(datos_semana) > 0:\n",
        "        datos_boxplot.append(datos_semana)\n",
        "        semanas_boxplot.append(semana)\n",
        "\n",
        "bp = axes[1].boxplot(datos_boxplot, positions=semanas_boxplot, widths=0.8,\n",
        "                      patch_artist=True, showfliers=True)\n",
        "\n",
        "# Colorear los boxplots\n",
        "for patch in bp['boxes']:\n",
        "    patch.set_facecolor(sns.color_palette(\"rocket\")[2])\n",
        "    patch.set_alpha(0.7)\n",
        "\n",
        "axes[1].set_xlabel('Semana del Año', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('Cantidad Vendida (kg)', fontsize=12, fontweight='bold')\n",
        "axes[1].set_title('Distribución por Semana (Todos los Años)', fontsize=13, pad=10)\n",
        "axes[1].set_xticks(range(1, 54, 2))\n",
        "axes[1].grid(True, alpha=0.3, linestyle='--', axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"patron_estacional_por_semana.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJOuzWg5gZWk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# Calcular ventas por año\n",
        "ventas_por_año = df_winsorizado.groupby('Año')['Cantidad_Vendida'].sum().reset_index()\n",
        "\n",
        "# Calcular crecimiento porcentual\n",
        "ventas_por_año['Crecimiento_%'] = ventas_por_año['Cantidad_Vendida'].pct_change() * 100\n",
        "\n",
        "# Obtener colores de la paleta rocket\n",
        "colores_rocket = sns.color_palette(\"rocket\", n_colors=5)\n",
        "color_principal = colores_rocket[2]  # Color medio de la paleta\n",
        "color_tendencia = colores_rocket[4]  # Color más intenso para la tendencia\n",
        "\n",
        "# Crear gráfico con más espacio vertical\n",
        "plt.figure(figsize=(14, 9))\n",
        "\n",
        "# Línea principal\n",
        "plt.plot(ventas_por_año['Año'], ventas_por_año['Cantidad_Vendida'],\n",
        "         marker='o', markersize=12, linewidth=3, color=color_principal,\n",
        "         label='Ventas Totales', markerfacecolor='white',\n",
        "         markeredgewidth=2.5, markeredgecolor=color_principal)\n",
        "\n",
        "# Calcular el máximo y mínimo para posicionar mejor los textos\n",
        "max_ventas = ventas_por_año['Cantidad_Vendida'].max()\n",
        "min_ventas = ventas_por_año['Cantidad_Vendida'].min()\n",
        "rango = max_ventas - min_ventas\n",
        "\n",
        "# Agregar valores en los puntos con porcentaje de crecimiento\n",
        "for i, (año, ventas, crecimiento) in enumerate(zip(ventas_por_año['Año'],\n",
        "                                                     ventas_por_año['Cantidad_Vendida'],\n",
        "                                                     ventas_por_año['Crecimiento_%'])):\n",
        "    # Crear el texto con ventas y crecimiento\n",
        "    if pd.notna(crecimiento):  # Si hay crecimiento calculado\n",
        "        color_crec = '#06A77D' if crecimiento > 0 else '#F95738'\n",
        "        texto = f'{ventas:,.0f} kg\\n({crecimiento:+.1f}%)'\n",
        "    else:  # Primer año (sin crecimiento)\n",
        "        color_crec = color_principal\n",
        "        texto = f'{ventas:,.0f} kg\\n(Base)'\n",
        "\n",
        "   # Alternar posición\n",
        "    if i % 2 == 0:  # Posición arriba\n",
        "        offset = max_ventas * 0.04\n",
        "        va = 'bottom'\n",
        "    else:  # Posición abajo\n",
        "        offset = -max_ventas * 0.03\n",
        "        va = 'top'\n",
        "    plt.text(año, ventas + offset,\n",
        "             texto, ha='center', va=va,\n",
        "             fontsize=9.5, fontweight='bold',\n",
        "             bbox=dict(boxstyle='round,pad=0.5', facecolor='white',\n",
        "                      edgecolor=color_crec, linewidth=2, alpha=0.95))\n",
        "\n",
        "plt.title('Evolución y Crecimiento de Ventas por Año',\n",
        "          fontsize=16, fontweight='bold', pad=30)\n",
        "plt.xlabel('Año', fontsize=13, fontweight='bold')\n",
        "plt.ylabel('Cantidad Vendida (kg)', fontsize=13, fontweight='bold')\n",
        "plt.legend(fontsize=11, loc='best', framealpha=0.9, edgecolor='black')\n",
        "plt.grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "# Ajustar límites del eje Y para dar más espacio a los textos\n",
        "plt.ylim(min_ventas - rango * 0.15, max_ventas + rango * 0.2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('evolucion_ventas_año.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeqnCzrjzH2V"
      },
      "source": [
        "**ANALISIS DE ESTACIONARIEDAD**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34HRxZOR6VfD"
      },
      "outputs": [],
      "source": [
        "df_winsorizado.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrXlgSuu00_z"
      },
      "outputs": [],
      "source": [
        "# Código para verificar solo Cantidad_Vendida\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "def test_estacionariedad_simple(serie, nombre):\n",
        "    resultado = adfuller(serie.dropna(), autolag='AIC')\n",
        "    return {\n",
        "        'Variable': nombre,\n",
        "        'Estadístico ADF': round(resultado[0], 4),\n",
        "        'Valor p': round(resultado[1], 4),\n",
        "        'Es Estacionaria': '✓ Sí' if resultado[1] < 0.05 else '✗ No',\n",
        "        'Acción Requerida': 'Ninguna' if resultado[1] < 0.05 else 'Aplicar diferenciación'\n",
        "    }\n",
        "\n",
        "# Analizar variable dependiente\n",
        "resultado_dep = test_estacionariedad_simple(df_winsorizado['Cantidad_Vendida'], 'Cantidad_Vendida (Dependiente)')\n",
        "\n",
        "\n",
        "print(\"ANÁLISIS DE ESTACIONARIEDAD PARA MODELO ARIMA datos windsorizado\")\n",
        "\n",
        "print(f\"\\nVariable: {resultado_dep['Variable']}\")\n",
        "print(f\"Estadístico ADF: {resultado_dep['Estadístico ADF']}\")\n",
        "print(f\"Valor p: {resultado_dep['Valor p']}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sURyPKtT3fTB"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# Calcular correlaciones, excluyendo columnas no numéricas como 'Año_Semana'\n",
        "columnas_numericas = df_winsorizado.select_dtypes(include=['float64', 'int64', 'int32', 'uint32']).columns\n",
        "corr_winsorizado_vendida = df_winsorizado[columnas_numericas].corr()['Cantidad_Vendida'].drop('Cantidad_Vendida')\n",
        "orden = corr_winsorizado_vendida.abs().sort_values(ascending=True).index\n",
        "\n",
        "# Gráfico Winsorizado\n",
        "sns.barplot(x=corr_winsorizado_vendida[orden].values, y=orden, palette=\"rocket\", ax=ax)\n",
        "ax.set_title('Correlaciones con Cantidad_Vendida - DataFrame Winsorizado',\n",
        "             fontsize=14, fontweight='bold', pad=15)\n",
        "ax.set_xlabel('Correlación (r)', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Variables', fontsize=12, fontweight='bold')\n",
        "ax.axvline(x=0, color='black', linewidth=1.5, alpha=0.3)\n",
        "ax.grid(axis='x', linestyle='--', alpha=0.4)\n",
        "\n",
        "# Añadir valores\n",
        "for i, value in enumerate(corr_winsorizado_vendida[orden].values):\n",
        "    ax.text(value + 0.01 if value >= 0 else value - 0.01, i,\n",
        "            f\"{value:.3f}\", va='center',\n",
        "            ha='left' if value >= 0 else 'right', fontsize=9, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"correlacion_winsorizado.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miMfSaZ2w6Zw"
      },
      "outputs": [],
      "source": [
        "# Código para verificar solo Cantidad_Vendida\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "def test_estacionariedad_simple(serie, nombre):\n",
        "    resultado = adfuller(serie.dropna(), autolag='AIC')\n",
        "    return {\n",
        "        'Variable': nombre,\n",
        "        'Estadístico ADF': round(resultado[0], 4),\n",
        "        'Valor p': round(resultado[1], 4),\n",
        "        'Es Estacionaria': '✓ Sí' if resultado[1] < 0.05 else '✗ No',\n",
        "        'Acción Requerida': 'Ninguna' if resultado[1] < 0.05 else 'Aplicar diferenciación'\n",
        "    }\n",
        "\n",
        "# Analizar variable dependiente\n",
        "resultado_dep = test_estacionariedad_simple(df_winsorizado['Cantidad_Vendida'], 'Cantidad_Vendida (Dependiente)')\n",
        "\n",
        "\n",
        "print(\"ANÁLISIS DE ESTACIONARIEDAD PARA MODELO ARIMA df_TOTAL\")\n",
        "\n",
        "print(f\"\\nVariable: {resultado_dep['Variable']}\")\n",
        "print(f\"Estadístico ADF: {resultado_dep['Estadístico ADF']}\")\n",
        "print(f\"Valor p: {resultado_dep['Valor p']}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOOzoqJQ8MTB"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Obtener colores de la paleta rocket\n",
        "colores_rocket = sns.color_palette(\"rocket\", n_colors=6)\n",
        "\n",
        "# Crear un índice numérico simple (número de semana/observación)\n",
        "df_temp = df_winsorizado.copy()\n",
        "df_temp['Observacion'] = range(1, len(df_temp) + 1)\n",
        "df_temp = df_temp.set_index('Observacion')\n",
        "\n",
        "# Realizar descomposición aditiva\n",
        "decomposition = seasonal_decompose(df_temp['Cantidad_Vendida'],\n",
        "                                   model='additive',\n",
        "                                   period=52,  # 52 semanas = 1 año de estacionalidad\n",
        "                                   extrapolate_trend='freq')\n",
        "\n",
        "# Crear el gráfico\n",
        "fig, axes = plt.subplots(4, 1, figsize=(16, 12))\n",
        "fig.suptitle('Descomposición de Serie Temporal Semanal - Cantidad Vendida (Winsorizado)',\n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "\n",
        "# Serie Original - Color rocket[0]\n",
        "axes[0].plot(decomposition.observed.index, decomposition.observed.values,\n",
        "             linewidth=1.5, color=colores_rocket[0])\n",
        "axes[0].set_ylabel('Original\\n(kg)', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('Serie Temporal Original (Datos Semanales)', fontsize=12, pad=10)\n",
        "axes[0].grid(True, alpha=0.3, linestyle='--')\n",
        "axes[0].set_xlabel('Semana (Observación)', fontsize=11)\n",
        "\n",
        "# Tendencia - Color rocket[2]\n",
        "axes[1].plot(decomposition.trend.index, decomposition.trend.values,\n",
        "             linewidth=2, color=colores_rocket[2])\n",
        "axes[1].set_ylabel('Tendencia\\n(kg)', fontsize=12, fontweight='bold')\n",
        "axes[1].set_title('Componente de Tendencia', fontsize=12, pad=10)\n",
        "axes[1].grid(True, alpha=0.3, linestyle='--')\n",
        "axes[1].set_xlabel('Semana (Observación)', fontsize=11)\n",
        "\n",
        "# Estacionalidad - Color rocket[3]\n",
        "axes[2].plot(decomposition.seasonal.index, decomposition.seasonal.values,\n",
        "             linewidth=1.5, color=colores_rocket[3])\n",
        "axes[2].set_ylabel('Estacionalidad\\n(kg)', fontsize=12, fontweight='bold')\n",
        "axes[2].set_title('Componente Estacional (Periodo: 52 semanas)', fontsize=12, pad=10)\n",
        "axes[2].grid(True, alpha=0.3, linestyle='--')\n",
        "axes[2].set_xlabel('Semana (Observación)', fontsize=11)\n",
        "\n",
        "# Residuos - Color rocket[4]\n",
        "axes[3].plot(decomposition.resid.index, decomposition.resid.values,\n",
        "             linewidth=1, color=colores_rocket[4], alpha=0.8)\n",
        "axes[3].axhline(y=0, color=colores_rocket[5], linestyle='--', linewidth=1.5, alpha=0.7)\n",
        "axes[3].set_ylabel('Residuos\\n(kg)', fontsize=12, fontweight='bold')\n",
        "axes[3].set_xlabel('Semana (Observación)', fontsize=12, fontweight='bold')\n",
        "axes[3].set_title('Componente Residual (Ruido)', fontsize=12, pad=10)\n",
        "axes[3].grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"descomposicion_serie_semanal.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Estadísticas de cada componente\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ESTADÍSTICAS DE LA DESCOMPOSICIÓN (DATOS SEMANALES)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "componentes = {\n",
        "    'Original': decomposition.observed,\n",
        "    'Tendencia': decomposition.trend,\n",
        "    'Estacionalidad': decomposition.seasonal,\n",
        "    'Residuos': decomposition.resid\n",
        "}\n",
        "\n",
        "print(f\"\\n{'Componente':<20} {'Media':>12} {'Desv.Std':>12} {'Min':>12} {'Max':>12}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for nombre, serie in componentes.items():\n",
        "    print(f\"{nombre:<20} {serie.mean():>12.2f} {serie.std():>12.2f} \"\n",
        "          f\"{serie.min():>12.2f} {serie.max():>12.2f}\")\n",
        "\n",
        "# Varianza explicada por cada componente\n",
        "var_tendencia = decomposition.trend.var()\n",
        "var_estacional = decomposition.seasonal.var()\n",
        "var_residual = decomposition.resid.var()\n",
        "var_total = var_tendencia + var_estacional + var_residual\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PROPORCIÓN DE VARIANZA EXPLICADA\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Tendencia:      {(var_tendencia/var_total)*100:>6.2f}%\")\n",
        "print(f\"Estacionalidad: {(var_estacional/var_total)*100:>6.2f}%\")\n",
        "print(f\"Residuos:       {(var_residual/var_total)*100:>6.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"INFORMACIÓN ADICIONAL\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total de semanas analizadas: {len(df_temp)}\")\n",
        "print(f\"Periodo estacional: 52 semanas (1 año)\")\n",
        "print(f\"Número de ciclos completos: {len(df_temp) / 52:.1f} años\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPHyy3slq8q9"
      },
      "source": [
        "**MODELO SARIMAX Y XG BOOST CON DATOS DF TOTAL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsHc7konrJv9"
      },
      "source": [
        "**MODELO SARIMAX Y XG BOOST CON DF_WINSORIZADO**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SC2IsEQxrzmc"
      },
      "outputs": [],
      "source": [
        "df_winsorizado.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkNNqFfKHblt"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "\n",
        "def test_estacionariedad(serie, nombre='Serie'):\n",
        "    \"\"\"\n",
        "    Realiza pruebas ADF y KPSS para evaluar estacionariedad\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ANÁLISIS DE ESTACIONARIEDAD: {nombre}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Test ADF (H0: serie NO es estacionaria)\n",
        "    resultado_adf = adfuller(serie.dropna(), autolag='AIC')\n",
        "    print(\"\\n1. TEST AUGMENTED DICKEY-FULLER (ADF):\")\n",
        "    print(f\"   Estadístico ADF: {resultado_adf[0]:.4f}\")\n",
        "    print(f\"   p-value: {resultado_adf[1]:.4f}\")\n",
        "    print(f\"   Valores críticos:\")\n",
        "    for key, value in resultado_adf[4].items():\n",
        "        print(f\"      {key}: {value:.4f}\")\n",
        "\n",
        "    if resultado_adf[1] < 0.05:\n",
        "        print(\"   ✓ Conclusión: Rechazamos H0 → La serie ES estacionaria (ADF)\")\n",
        "    else:\n",
        "        print(\"   ✗ Conclusión: No rechazamos H0 → La serie NO es estacionaria (ADF)\")\n",
        "\n",
        "    # Test KPSS (H0: serie ES estacionaria)\n",
        "    resultado_kpss = kpss(serie.dropna(), regression='ct', nlags='auto')\n",
        "    print(\"\\n2. TEST KPSS:\")\n",
        "    print(f\"   Estadístico KPSS: {resultado_kpss[0]:.4f}\")\n",
        "    print(f\"   p-value: {resultado_kpss[1]:.4f}\")\n",
        "    print(f\"   Valores críticos:\")\n",
        "    for key, value in resultado_kpss[3].items():\n",
        "        print(f\"      {key}: {value:.4f}\")\n",
        "\n",
        "    if resultado_kpss[1] > 0.05:\n",
        "        print(\"   ✓ Conclusión: No rechazamos H0 → La serie ES estacionaria (KPSS)\")\n",
        "    else:\n",
        "        print(\"   ✗ Conclusión: Rechazamos H0 → La serie NO es estacionaria (KPSS)\")\n",
        "\n",
        "    return resultado_adf[1], resultado_kpss[1]\n",
        "\n",
        "# Analizar serie original\n",
        "p_adf, p_kpss = test_estacionariedad(df_winsorizado['Cantidad_Vendida'], 'Cantidad Vendida Original')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rljgTpFPiFiv"
      },
      "outputs": [],
      "source": [
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Serie original (YA estacionaria según tu primer gráfico)\n",
        "serie_original = df_winsorizado['Cantidad_Vendida'].copy()\n",
        "\n",
        "# Para ARIMA: usa la serie original (d=0)\n",
        "serie_arima = serie_original.copy()  # SIN diferenciar\n",
        "\n",
        "# Para SARIMAX: también usa serie original primero\n",
        "# (solo diferencia si detectas estacionalidad fuerte)\n",
        "s = 52\n",
        "serie_sarimax = serie_original.copy()  # SIN diferenciar\n",
        "# Si necesitas diferenciación estacional:\n",
        "# serie_sarimax = serie_original.diff(s).dropna()  # Solo D=1\n",
        "\n",
        "# Crear figura con 4 subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "\n",
        "# ARIMA - ACF\n",
        "max_lags_arima = min(52, len(serie_arima) // 2)\n",
        "plot_acf(serie_arima, lags=max_lags_arima, ax=axes[0, 0], alpha=0.05)\n",
        "axes[0, 0].set_title('ARIMA - ACF (d=0)', fontsize=13, fontweight='bold')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# ARIMA - PACF\n",
        "plot_pacf(serie_arima, lags=max_lags_arima, ax=axes[0, 1], alpha=0.05, method='ywm')\n",
        "axes[0, 1].set_title('ARIMA - PACF (d=0)', fontsize=13, fontweight='bold')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# SARIMAX - ACF\n",
        "max_lags_sarimax = min(s * 2, len(serie_sarimax) // 2)\n",
        "plot_acf(serie_sarimax, lags=max_lags_sarimax, ax=axes[1, 0], alpha=0.05)\n",
        "axes[1, 0].set_title(f'SARIMAX - ACF (s={s}, d=0, D=0)', fontsize=13, fontweight='bold')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "\n",
        "# SARIMAX - PACF\n",
        "plot_pacf(serie_sarimax, lags=max_lags_sarimax, ax=axes[1, 1], alpha=0.05, method='ywm')\n",
        "axes[1, 1].set_title(f'SARIMAX - PACF (s={s}, d=0, D=0)', fontsize=13, fontweight='bold')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=== COMPARACIÓN ===\")\n",
        "print(f\"ARIMA analiza lags: 1 a {max_lags_arima} (d=0)\")\n",
        "print(f\"SARIMAX analiza lags: 1 a {max_lags_sarimax} (d=0, D=0)\")\n",
        "print(\"\\n⚠️ Si los gráficos muestran valores negativos fuertes,\")\n",
        "print(\"   significa que aplicaste diferenciación innecesaria\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aytpGk7jZy9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# ============================================\n",
        "# DETECTAR ESTACIONALIDAD EN TUS DATOS\n",
        "# ============================================\n",
        "\n",
        "serie = df_winsorizado['Cantidad_Vendida'].copy()\n",
        "\n",
        "# Asumiendo que tienes un índice de fecha\n",
        "# Si no lo tienes, créalo:\n",
        "# df_winsorizado['Fecha'] = pd.date_range(start='2020-01-01', periods=len(serie), freq='W')\n",
        "\n",
        "print(\"🔍 ANÁLISIS VISUAL PARA DETECTAR ESTACIONALIDAD:\\n\")\n",
        "\n",
        "# Gráfico 1: Serie temporal completa\n",
        "fig, axes = plt.subplots(4, 1, figsize=(16, 14))\n",
        "\n",
        "# Serie completa\n",
        "axes[0].plot(serie.index, serie, linewidth=1)\n",
        "axes[0].set_title('Serie Temporal Completa - Busca patrones repetitivos',\n",
        "                  fontsize=13, fontweight='bold')\n",
        "axes[0].set_ylabel('Cantidad Vendida')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Descomposición por año (si tienes 4 años)\n",
        "# Esto te ayuda a ver si el patrón se repite cada año\n",
        "if len(serie) >= 52*2:  # Al menos 2 años de datos\n",
        "    años = len(serie) // 52\n",
        "    colores = ['blue', 'green', 'red', 'orange', 'purple']\n",
        "\n",
        "    axes[1].set_title('Comparación por Año - ¿Se repite el patrón anualmente?',\n",
        "                      fontsize=13, fontweight='bold')\n",
        "    for año in range(min(años, 4)):\n",
        "        inicio = año * 52\n",
        "        fin = min((año + 1) * 52, len(serie))\n",
        "        semanas = range(1, fin - inicio + 1)\n",
        "        axes[1].plot(semanas, serie.iloc[inicio:fin].values,\n",
        "                    label=f'Año {año+1}', color=colores[año], alpha=0.7)\n",
        "    axes[1].set_xlabel('Semana del Año')\n",
        "    axes[1].set_ylabel('Cantidad Vendida')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Promedio por semana del año (detectar estacionalidad anual)\n",
        "if len(serie) >= 52*2:\n",
        "    serie_df = pd.DataFrame({\n",
        "        'valor': serie.values,\n",
        "        'semana_año': [(i % 52) + 1 for i in range(len(serie))]\n",
        "    })\n",
        "    promedio_semanal = serie_df.groupby('semana_año')['valor'].mean()\n",
        "\n",
        "    axes[2].plot(promedio_semanal.index, promedio_semanal.values,\n",
        "                linewidth=2, marker='o', markersize=3, color='darkblue')\n",
        "    axes[2].set_title('Promedio por Semana del Año - ¿Hay patrón anual (s=52)?',\n",
        "                      fontsize=13, fontweight='bold')\n",
        "    axes[2].set_xlabel('Semana del Año (1-52)')\n",
        "    axes[2].set_ylabel('Cantidad Vendida Promedio')\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "    axes[2].axhline(y=promedio_semanal.mean(), color='red',\n",
        "                    linestyle='--', label='Media general')\n",
        "    axes[2].legend()\n",
        "\n",
        "# Boxplot por semana del año\n",
        "if len(serie) >= 52*2:\n",
        "    # Agrupar cada 4 semanas para ver patrón mensual\n",
        "    serie_df['mes'] = [(i % 52) // 4 + 1 for i in range(len(serie))]\n",
        "\n",
        "    datos_boxplot = [serie_df[serie_df['mes'] == m]['valor'].values\n",
        "                     for m in range(1, 14)]\n",
        "    axes[3].boxplot(datos_boxplot, labels=range(1, 14))\n",
        "    axes[3].set_title('Distribución por \"Mes\" (cada 4 semanas) - ¿Hay patrón mensual (s=4)?',\n",
        "                      fontsize=13, fontweight='bold')\n",
        "    axes[3].set_xlabel('Mes del Año (agrupado cada 4 semanas)')\n",
        "    axes[3].set_ylabel('Cantidad Vendida')\n",
        "    axes[3].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n📌 INTERPRETA LOS GRÁFICOS:\")\n",
        "print(\"   - Gráfico 2: Si las líneas siguen un patrón similar → s=52 (anual)\")\n",
        "print(\"   - Gráfico 3: Si hay picos/valles claros en ciertas semanas → s=52 (anual)\")\n",
        "print(\"   - Gráfico 4: Si hay diferencias entre meses → podría ser s=4 (mensual)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODELO ARIMA EVALUACIÓN**\n"
      ],
      "metadata": {
        "id": "7pNaiUQx3By3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLhe2q-QPmax"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from scipy import stats\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Preparar los datos\n",
        "y = df_winsorizado['Cantidad_Vendida']\n",
        "train_size = int(len(y) * 0.8)\n",
        "train, test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Órdenes de ARIMA a probar (basados en ACF/PACF)\n",
        "ordenes_arima = [\n",
        "    (1, 0, 1),  # ARMA(1,1) - modelo mixto simple\n",
        "    (1, 0, 2),  # ARMA(1,2) - más complejo en MA\n",
        "    (2, 0, 1),  # ARMA(2,1) - más complejo en AR\n",
        "    (2, 0, 2),  # ARMA(2,2) - tu modelo actual\n",
        "    (3, 0, 1),  # ARMA(3,1) - AR más complejo\n",
        "    (3, 0, 2),  # ARMA(3,2) - ambos complejos\n",
        "]\n",
        "\n",
        "# Diccionario para almacenar resultados\n",
        "resultados = []\n",
        "\n",
        "print(\"=\"*100)\n",
        "print(\"COMPARACIÓN DE MODELOS ARIMA\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "for orden in ordenes_arima:\n",
        "    try:\n",
        "        # Ajustar modelo\n",
        "        modelo = ARIMA(train, order=orden)\n",
        "        modelo_fit = modelo.fit()\n",
        "\n",
        "        # Predicciones\n",
        "        pred = modelo_fit.forecast(steps=len(test))\n",
        "\n",
        "        # Métricas de ajuste\n",
        "        aic = modelo_fit.aic\n",
        "        bic = modelo_fit.bic\n",
        "        hqic = modelo_fit.hqic\n",
        "\n",
        "        # Métricas de predicción\n",
        "        mae = mean_absolute_error(test, pred)\n",
        "        rmse = np.sqrt(mean_squared_error(test, pred))\n",
        "        mape = np.mean(np.abs((test - pred) / test)) * 100\n",
        "\n",
        "        # Test de Ljung-Box\n",
        "        lb_test = acorr_ljungbox(modelo_fit.resid, lags=[10], return_df=True)\n",
        "        lb_pvalue = lb_test['lb_pvalue'].values[0]\n",
        "\n",
        "        # Test de Jarque-Bera\n",
        "        jb_stat, jb_pvalue = stats.jarque_bera(modelo_fit.resid)\n",
        "\n",
        "        # Skewness y Kurtosis\n",
        "        skew = stats.skew(modelo_fit.resid)\n",
        "        kurt = stats.kurtosis(modelo_fit.resid, fisher=False)\n",
        "\n",
        "        # Guardar resultados\n",
        "        resultados.append({\n",
        "            'Orden': f'ARIMA{orden}',\n",
        "            'AIC': aic,\n",
        "            'BIC': bic,\n",
        "            'HQIC': hqic,\n",
        "            'MAE': mae,\n",
        "            'RMSE': rmse,\n",
        "            'MAPE': mape,\n",
        "            'LB_pvalue': lb_pvalue,\n",
        "            'JB_pvalue': jb_pvalue,\n",
        "            'Skewness': skew,\n",
        "            'Kurtosis': kurt,\n",
        "            'N_params': len(modelo_fit.params)\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error en ARIMA{orden}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "# Crear DataFrame con resultados\n",
        "df_resultados = pd.DataFrame(resultados)\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"TABLA COMPLETA DE RESULTADOS\")\n",
        "print(\"=\"*100)\n",
        "print(df_resultados.to_string(index=False))\n",
        "\n",
        "# Visualización comparativa de los top 5\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "top5_modelos = df_resultados.nsmallest(5, 'AIC')\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Gráfico 1: AIC vs BIC\n",
        "axes[0, 0].scatter(top5_modelos['AIC'], top5_modelos['BIC'], s=100, alpha=0.6)\n",
        "for idx, row in top5_modelos.iterrows():\n",
        "    axes[0, 0].annotate(row['Orden'], (row['AIC'], row['BIC']), fontsize=9)\n",
        "axes[0, 0].set_xlabel('AIC', fontsize=11)\n",
        "axes[0, 0].set_ylabel('BIC', fontsize=11)\n",
        "axes[0, 0].set_title('Top 5: AIC vs BIC', fontweight='bold')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Gráfico 2: MAE comparación\n",
        "axes[0, 1].barh(top5_modelos['Orden'], top5_modelos['MAE'], color='steelblue', alpha=0.7)\n",
        "axes[0, 1].set_xlabel('MAE', fontsize=11)\n",
        "axes[0, 1].set_title('Top 5: Error Absoluto Medio', fontweight='bold')\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Gráfico 3: MAPE comparación\n",
        "axes[1, 0].barh(top5_modelos['Orden'], top5_modelos['MAPE'], color='coral', alpha=0.7)\n",
        "axes[1, 0].set_xlabel('MAPE (%)', fontsize=11)\n",
        "axes[1, 0].set_title('Top 5: Error Porcentual', fontweight='bold')\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Gráfico 4: Diagnóstico (Ljung-Box y Jarque-Bera p-values)\n",
        "x = np.arange(len(top5_modelos))\n",
        "width = 0.35\n",
        "axes[1, 1].bar(x - width/2, top5_modelos['LB_pvalue'], width, label='Ljung-Box p-value', alpha=0.7)\n",
        "axes[1, 1].bar(x + width/2, top5_modelos['JB_pvalue'], width, label='Jarque-Bera p-value', alpha=0.7)\n",
        "axes[1, 1].axhline(y=0.05, color='r', linestyle='--', label='p=0.05 (umbral)')\n",
        "axes[1, 1].set_ylabel('p-value', fontsize=11)\n",
        "axes[1, 1].set_title('Top 5: Tests de Diagnóstico', fontweight='bold')\n",
        "axes[1, 1].set_xticks(x)\n",
        "axes[1, 1].set_xticklabels(top5_modelos['Orden'], rotation=45)\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ENTRENAMIENTO MODELO ARIMA**\n"
      ],
      "metadata": {
        "id": "rEQQ8KNr4eye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# 1. PREPARACIÓN DE DATOS\n",
        "\n",
        "\n",
        "# Asignar la columna 'Cantidad_Vendida' de df_winsorizado a ventas\n",
        "ventas = df_winsorizado['Cantidad_Vendida']\n",
        "\n",
        "# Dividir en entrenamiento (80%) y prueba (20%)\n",
        "train_size = int(len(ventas) * 0.8)\n",
        "train = ventas[:train_size]\n",
        "test = ventas[train_size:]\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PREPARACIÓN DE DATOS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Datos de entrenamiento: {len(train)} semanas\")\n",
        "print(f\"Datos de prueba: {len(test)} semanas\")\n",
        "print(f\"Total de observaciones: {len(ventas)} semanas\")\n",
        "\n",
        "\n",
        "# 2. AJUSTAR MODELO ARIMA\n",
        "\n",
        "\n",
        "# Definir el orden (p, d, q)\n",
        "# Basado en tu análisis previo, estos son tus modelos seleccionados:\n",
        "# - Modelo Principal: ARIMA(3,0,2) - Mejor validación estadística\n",
        "# - Modelo Alternativo: ARIMA(1,0,1) - Más simple\n",
        "\n",
        "# Puedes cambiar entre (3,0,2) o (1,0,1) según necesites\n",
        "order = (3, 0, 2)  # Modelo principal recomendado\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"AJUSTANDO MODELO ARIMA{order}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Ajustar el modelo en los datos de entrenamiento\n",
        "modelo = ARIMA(train, order=order)\n",
        "modelo_fit = modelo.fit()\n",
        "\n",
        "print(\"\\n✓ Modelo ajustado exitosamente\")\n",
        "print(\"\\nResumen del modelo:\")\n",
        "print(modelo_fit.summary())\n",
        "\n",
        "\n",
        "# 3. PREDICCIONES EN CONJUNTO DE PRUEBA\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERANDO PREDICCIONES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "predicciones = modelo_fit.forecast(steps=len(test))\n",
        "\n",
        "print(f\"✓ {len(predicciones)} predicciones generadas\")\n",
        "\n",
        "\n",
        "# 4. EVALUACIÓN DEL MODELO\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUACIÓN DEL MODELO ARIMA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calcular métricas de error\n",
        "mae = mean_absolute_error(test, predicciones)\n",
        "rmse = np.sqrt(mean_squared_error(test, predicciones))\n",
        "mape = np.mean(np.abs((test - predicciones) / test)) * 100\n",
        "\n",
        "print(f\"\\nMétricas de Desempeño:\")\n",
        "print(f\"  • Error Absoluto Medio (MAE):           {mae:.2f} kg\")\n",
        "print(f\"  • Raíz del Error Cuadrático Medio (RMSE): {rmse:.2f} kg\")\n",
        "print(f\"  • Error Porcentual Absoluto Medio (MAPE):  {mape:.2f}%\")\n",
        "\n",
        "# Estadísticas adicionales\n",
        "print(f\"\\nEstadísticas de Predicción:\")\n",
        "print(f\"  • Predicción promedio:  {predicciones.mean():.2f} kg\")\n",
        "print(f\"  • Real promedio:        {test.mean():.2f} kg\")\n",
        "print(f\"  • Diferencia:           {predicciones.mean() - test.mean():.2f} kg\")\n",
        "\n",
        "\n",
        "# 5. VISUALIZACIÓN DE RESULTADOS\n",
        "\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Datos de entrenamiento\n",
        "plt.plot(train.index, train, label='Datos de Entrenamiento',\n",
        "         color='purple', linewidth=1.5, alpha=0.7)\n",
        "\n",
        "# Datos reales de prueba\n",
        "plt.plot(test.index, test, label='Datos Reales (Prueba)',\n",
        "         color='coral', linewidth=2.5, marker='o', markersize=4)\n",
        "\n",
        "# Predicciones\n",
        "plt.plot(test.index, predicciones, label=f'Predicciones ARIMA{order}',\n",
        "         color='red', linestyle='--', linewidth=2.5, marker='s', markersize=4)\n",
        "\n",
        "# Línea vertical separando train/test\n",
        "plt.axvline(x=train.index[-1], color='gray', linestyle=':',\n",
        "            linewidth=2, label='Inicio Predicción', alpha=0.7)\n",
        "\n",
        "plt.title(f'Predicciones ARIMA{order} vs Datos Reales',\n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Índice de Observación', fontsize=12)\n",
        "plt.ylabel('Cantidad Vendida (kg)', fontsize=12)\n",
        "plt.legend(loc='best', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 6. DIAGNÓSTICO DE RESIDUOS\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DIAGNÓSTICO DE RESIDUOS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Gráficos de diagnóstico\n",
        "fig = modelo_fit.plot_diagnostics(figsize=(15, 10))\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Análisis de residuos\n",
        "residuos = modelo_fit.resid\n",
        "\n",
        "print(f\"\\nEstadísticas de Residuos:\")\n",
        "print(f\"  • Media:               {residuos.mean():.4f}\")\n",
        "print(f\"  • Desviación estándar: {residuos.std():.2f}\")\n",
        "print(f\"  • Mínimo:              {residuos.min():.2f}\")\n",
        "print(f\"  • Máximo:              {residuos.max():.2f}\")\n",
        "\n",
        "\n",
        "# 7. COMPARACIÓN CON MODELO ALTERNATIVO (OPCIONAL)\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARACIÓN CON MODELO ALTERNATIVO ARIMA(1,0,1)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Ajustar modelo alternativo\n",
        "order_alt = (1, 0, 1)\n",
        "modelo_alt = ARIMA(train, order=order_alt)\n",
        "modelo_alt_fit = modelo_alt.fit()\n",
        "\n",
        "# Predicciones del modelo alternativo\n",
        "predicciones_alt = modelo_alt_fit.forecast(steps=len(test))\n",
        "\n",
        "# Evaluar modelo alternativo\n",
        "mae_alt = mean_absolute_error(test, predicciones_alt)\n",
        "rmse_alt = np.sqrt(mean_squared_error(test, predicciones_alt))\n",
        "mape_alt = np.mean(np.abs((test - predicciones_alt) / test)) * 100\n",
        "\n",
        "print(f\"\\nARIMA{order} (Principal):\")\n",
        "print(f\"  MAE:  {mae:.2f} kg | RMSE: {rmse:.2f} kg | MAPE: {mape:.2f}%\")\n",
        "\n",
        "print(f\"\\nARIMA{order_alt} (Alternativo):\")\n",
        "print(f\"  MAE:  {mae_alt:.2f} kg | RMSE: {rmse_alt:.2f} kg | MAPE: {mape_alt:.2f}%\")\n",
        "\n",
        "# Determinar el mejor modelo\n",
        "if mae < mae_alt:\n",
        "    print(f\"\\n✓ El modelo ARIMA{order} tiene mejor desempeño (menor MAE)\")\n",
        "else:\n",
        "    print(f\"\\n✓ El modelo ARIMA{order_alt} tiene mejor desempeño (menor MAE)\")\n",
        "\n",
        "\n",
        "# 8. GUARDAR PREDICCIONES\n",
        "\n",
        "\n",
        "# Crear DataFrame con resultados\n",
        "df_resultados = pd.DataFrame({\n",
        "    'Real': test.values,\n",
        "    'Prediccion_ARIMA': predicciones.values,\n",
        "    'Error': test.values - predicciones.values,\n",
        "    'Error_Porcentual': ((test.values - predicciones.values) / test.values) * 100\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PRIMERAS 10 PREDICCIONES\")\n",
        "print(\"=\"*80)\n",
        "print(df_resultados.head(10))\n",
        "\n",
        "# Opcional: guardar a CSV\n",
        "# df_resultados.to_csv('predicciones_arima.csv', index=False)\n",
        "# print(\"\\n✓ Resultados guardados en 'predicciones_arima.csv'\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANÁLISIS COMPLETADO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 9. PRONÓSTICO FUTURO (próximas 4 semanas) con ARIMA\n",
        "\n",
        "\n",
        "# 9.1 Reentrenar con toda la serie y pronosticar 4 pasos\n",
        "modelo_full = ARIMA(ventas, order=order).fit()\n",
        "forecast_4w = modelo_full.get_forecast(steps=4)\n",
        "y_future_4w = forecast_4w.predicted_mean\n",
        "conf_int   = forecast_4w.conf_int(alpha=0.05)  # 95%\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PRONÓSTICO ARIMA (próximas 4 semanas)\")\n",
        "print(\"=\"*80)\n",
        "print(y_future_4w.round(2))\n",
        "\n",
        "# 9.2 Construir índice temporal futuro\n",
        "use_dates = None\n",
        "if 'Fecha' in df_winsorizado.columns and np.issubdtype(df_winsorizado['Fecha'].dtype, np.datetime64):\n",
        "    last_date = df_winsorizado['Fecha'].iloc[-1]\n",
        "    future_index = pd.date_range(last_date + pd.Timedelta(weeks=1), periods=4, freq='W')\n",
        "    df_future = pd.DataFrame({'Fecha': future_index, 'Predicción': y_future_4w})\n",
        "    conf_int.index = future_index\n",
        "    x_start_line = future_index[0]\n",
        "    x_span_end   = future_index[-1]\n",
        "    x_hist = df_winsorizado[['Fecha','Cantidad_Vendida']]\n",
        "    use_dates = True\n",
        "elif 'Año_Semana' in df_winsorizado.columns:\n",
        "    def next_year_week(ys):\n",
        "        y, w = map(int, str(ys).split('-'))\n",
        "        w += 1\n",
        "        if w > 53: w = 1; y += 1\n",
        "        if w > 52: w = 1; y += 1\n",
        "        return f\"{y}-{w:02d}\"\n",
        "    start_ys = df_winsorizado['Año_Semana'].iloc[-1]\n",
        "    ys = []\n",
        "    cur = start_ys\n",
        "    for _ in range(4):\n",
        "        cur = next_year_week(cur); ys.append(cur)\n",
        "    df_future = pd.DataFrame({'Año_Semana': ys, 'Predicción': y_future_4w})\n",
        "    conf_int.index = ys\n",
        "    x_start_line = ys[0]; x_span_end = ys[-1]\n",
        "    x_hist = df_winsorizado[['Año_Semana','Cantidad_Vendida']]\n",
        "    use_dates = False\n",
        "else:\n",
        "    base = len(df_winsorizado)\n",
        "    idx = np.arange(base, base+4)\n",
        "    df_future = pd.DataFrame({'idx': idx, 'Predicción': y_future_4w})\n",
        "    conf_int.index = idx\n",
        "    x_start_line = idx[0]; x_span_end = idx[-1]\n",
        "    x_hist = pd.DataFrame({'idx': np.arange(len(df_winsorizado)),\n",
        "                           'Cantidad_Vendida': df_winsorizado['Cantidad_Vendida']})\n",
        "    use_dates = None\n",
        "\n",
        "# 9.3 Gráfico estil\n"
      ],
      "metadata": {
        "id": "L-CtIY2u5n-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opjljB8yRh5o"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from scipy import stats\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# 1. PREPARACIÓN DE DATOS\n",
        "\n",
        "\n",
        "# Variable dependiente\n",
        "y = df_winsorizado['Cantidad_Vendida']\n",
        "\n",
        "# Variables exógenas\n",
        "exog_vars = [\n",
        "    'Cantidad_Producida',\n",
        "    'Mermas',\n",
        "    'Inventario',\n",
        "    'Precio_Promedio',\n",
        "    'Es_Feriado',\n",
        "    'Pre_Feriado',\n",
        "    'Post_Feriado'\n",
        "]\n",
        "\n",
        "X = df_winsorizado[exog_vars]\n",
        "\n",
        "# Verificar que no hay valores nulos\n",
        "print(\"=\"*80)\n",
        "print(\"VERIFICACIÓN DE DATOS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nValores nulos en Y: {y.isnull().sum()}\")\n",
        "print(f\"Valores nulos en X:\\n{X.isnull().sum()}\")\n",
        "print(f\"\\nDimensiones:\")\n",
        "print(f\"Y: {y.shape}\")\n",
        "print(f\"X: {X.shape}\")\n",
        "\n",
        "# Dividir en train/test (80-20)\n",
        "train_size = int(len(y) * 0.8)\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "\n",
        "print(f\"\\nTamaño conjunto entrenamiento: {len(y_train)}\")\n",
        "print(f\"Tamaño conjunto prueba: {len(y_test)}\")\n",
        "\n",
        "\n",
        "# 3. FUNCIÓN PARA AJUSTAR Y EVALUAR MODELOS SARIMAX\n",
        "\n",
        "\n",
        "def ajustar_sarimax(y_train, y_test, X_train, X_test, order, seasonal_order=(0,0,0,0),\n",
        "                    nombre_modelo=\"SARIMAX\"):\n",
        "    \"\"\"\n",
        "    Ajusta un modelo SARIMAX y retorna métricas de evaluación\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Ajustar modelo\n",
        "        modelo = SARIMAX(\n",
        "            y_train,\n",
        "            exog=X_train,\n",
        "            order=order,\n",
        "            seasonal_order=seasonal_order,\n",
        "            enforce_stationarity=False,\n",
        "            enforce_invertibility=False\n",
        "        )\n",
        "\n",
        "        modelo_fit = modelo.fit(disp=False, maxiter=200)\n",
        "\n",
        "        # Predicciones\n",
        "        pred = modelo_fit.forecast(steps=len(y_test), exog=X_test)\n",
        "\n",
        "        # Métricas\n",
        "        mae = mean_absolute_error(y_test, pred)\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
        "        mape = np.mean(np.abs((y_test - pred) / y_test)) * 100\n",
        "\n",
        "        # Diagnóstico de residuos\n",
        "        residuos = modelo_fit.resid\n",
        "        lb_test = acorr_ljungbox(residuos, lags=[10], return_df=True)\n",
        "        lb_pvalue = lb_test['lb_pvalue'].values[0]\n",
        "\n",
        "        jb_stat, jb_pvalue = stats.jarque_bera(residuos)\n",
        "        skew = stats.skew(residuos)\n",
        "        kurt = stats.kurtosis(residuos, fisher=False)\n",
        "\n",
        "        return {\n",
        "            'modelo_fit': modelo_fit,\n",
        "            'predicciones': pred,\n",
        "            'orden': order,\n",
        "            'seasonal_order': seasonal_order,\n",
        "            'AIC': modelo_fit.aic,\n",
        "            'BIC': modelo_fit.bic,\n",
        "            'MAE': mae,\n",
        "            'RMSE': rmse,\n",
        "            'MAPE': mape,\n",
        "            'LB_pvalue': lb_pvalue,\n",
        "            'JB_pvalue': jb_pvalue,\n",
        "            'Skewness': skew,\n",
        "            'Kurtosis': kurt,\n",
        "            'N_params': len(modelo_fit.params)\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error en {nombre_modelo}{order}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# 4. PROBAR DIFERENTES ÓRDENES DE SARIMAX\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUACIÓN DE MODELOS SARIMAX CON VARIABLES EXÓGENAS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Órdenes a probar (basados en los mejores ARIMA previos)\n",
        "ordenes_sarimax = [\n",
        "    (1, 0, 2),\n",
        "    (2, 0, 2),\n",
        "    (3, 0, 1),\n",
        "    (3, 0, 2),\n",
        "]\n",
        "\n",
        "resultados_sarimax = []\n",
        "\n",
        "for orden in ordenes_sarimax:\n",
        "    print(f\"\\nAjustando SARIMAX{orden}...\", end=\" \")\n",
        "    resultado = ajustar_sarimax(y_train, y_test, X_train, X_test, orden)\n",
        "\n",
        "    if resultado:\n",
        "        resultados_sarimax.append(resultado)\n",
        "        print(f\"✓ AIC: {resultado['AIC']:.2f}, MAE: {resultado['MAE']:.2f}\")\n",
        "    else:\n",
        "        print(\"✗ Error\")\n",
        "\n",
        "# 5. COMPARACIÓN DE RESULTADOS\n",
        "\n",
        "\n",
        "df_resultados_sarimax = pd.DataFrame([\n",
        "    {k: v for k, v in r.items() if k not in ['modelo_fit', 'predicciones']}\n",
        "    for r in resultados_sarimax\n",
        "])\n",
        "\n",
        "df_resultados_sarimax['Orden'] = df_resultados_sarimax.apply(\n",
        "    lambda row: f\"SARIMAX{row['orden']}\", axis=1\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RANKING DE MODELOS SARIMAX\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Top 5 por AIC\n",
        "print(\"\\nTOP 5 POR AIC:\")\n",
        "top5_aic = df_resultados_sarimax.nsmallest(5, 'AIC')[\n",
        "    ['Orden', 'AIC', 'BIC', 'MAE', 'RMSE', 'MAPE', 'LB_pvalue', 'JB_pvalue']\n",
        "]\n",
        "print(top5_aic.to_string(index=False))\n",
        "\n",
        "# Top 5 por MAE\n",
        "print(\"\\nTOP 5 POR MAE:\")\n",
        "top5_mae = df_resultados_sarimax.nsmallest(5, 'MAE')[\n",
        "    ['Orden', 'AIC', 'BIC', 'MAE', 'RMSE', 'MAPE', 'LB_pvalue']\n",
        "]\n",
        "print(top5_mae.to_string(index=False))\n",
        "\n",
        "# Top 5 por MAPE\n",
        "print(\"\\nTOP 5 POR MAPE:\")\n",
        "top5_mape = df_resultados_sarimax.nsmallest(5, 'MAPE')[\n",
        "    ['Orden', 'AIC', 'MAE', 'MAPE', 'LB_pvalue', 'JB_pvalue']\n",
        "]\n",
        "print(top5_mape.to_string(index=False))\n",
        "\n",
        "\n",
        "# 6. SELECCIÓN DEL MEJOR MODELO\n",
        "\n",
        "\n",
        "# Filtrar modelos con residuos no autocorrelacionados\n",
        "df_buenos = df_resultados_sarimax[df_resultados_sarimax['LB_pvalue'] > 0.05]\n",
        "\n",
        "if len(df_buenos) > 0:\n",
        "    mejor_modelo_info = df_buenos.nsmallest(1, 'AIC').iloc[0]\n",
        "else:\n",
        "    print(\"\\n⚠️ Ningún modelo cumple con Ljung-Box. Seleccionando mejor por AIC...\")\n",
        "    mejor_modelo_info = df_resultados_sarimax.nsmallest(1, 'AIC').iloc[0]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🏆 MEJOR MODELO SARIMAX\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nModelo: {mejor_modelo_info['Orden']}\")\n",
        "print(f\"AIC: {mejor_modelo_info['AIC']:.2f}\")\n",
        "print(f\"BIC: {mejor_modelo_info['BIC']:.2f}\")\n",
        "print(f\"MAE: {mejor_modelo_info['MAE']:.2f} kg\")\n",
        "print(f\"RMSE: {mejor_modelo_info['RMSE']:.2f} kg\")\n",
        "print(f\"MAPE: {mejor_modelo_info['MAPE']:.2f}%\")\n",
        "print(f\"Ljung-Box p-value: {mejor_modelo_info['LB_pvalue']:.3f} {'✅' if mejor_modelo_info['LB_pvalue'] > 0.05 else '⚠️'}\")\n",
        "print(f\"Jarque-Bera p-value: {mejor_modelo_info['JB_pvalue']:.3f} {'✅' if mejor_modelo_info['JB_pvalue'] > 0.05 else '⚠️'}\")\n",
        "print(f\"Skewness: {mejor_modelo_info['Skewness']:.3f}\")\n",
        "print(f\"Kurtosis: {mejor_modelo_info['Kurtosis']:.3f}\")\n",
        "print(f\"Número de parámetros: {mejor_modelo_info['N_params']}\")\n",
        "\n",
        "# 7. ANÁLISIS DETALLADO DEL MEJOR MODELO\n",
        "\n",
        "\n",
        "# Obtener el mejor modelo ajustado\n",
        "mejor_idx = df_resultados_sarimax['AIC'].idxmin()\n",
        "mejor_modelo_fit = resultados_sarimax[mejor_idx]['modelo_fit']\n",
        "mejor_pred = resultados_sarimax[mejor_idx]['predicciones']\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESUMEN DETALLADO DEL MEJOR MODELO\")\n",
        "print(\"=\"*80)\n",
        "print(mejor_modelo_fit.summary())\n",
        "\n",
        "# Significancia de variables exógenas\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SIGNIFICANCIA DE VARIABLES EXÓGENAS (p < 0.05)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "params_df = pd.DataFrame({\n",
        "    'Variable': mejor_modelo_fit.params.index,\n",
        "    'Coeficiente': mejor_modelo_fit.params.values,\n",
        "    'Std Error': mejor_modelo_fit.bse.values,\n",
        "    'p-value': mejor_modelo_fit.pvalues.values,\n",
        "    'Significativo': ['✅ Sí' if p < 0.05 else '❌ No' for p in mejor_modelo_fit.pvalues.values]\n",
        "})\n",
        "\n",
        "# Filtrar solo las variables exógenas\n",
        "exog_params = params_df[params_df['Variable'].isin(exog_vars)]\n",
        "print(exog_params.to_string(index=False))\n",
        "\n",
        "\n",
        "# 8. VISUALIZACIONES\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "# 1. Predicciones vs Real\n",
        "axes[0, 0].plot(y_train.index, y_train, label='Entrenamiento', color='blue', alpha=0.7)\n",
        "axes[0, 0].plot(y_test.index, y_test, label='Real (Test)', color='green', linewidth=2, marker='o', markersize=4)\n",
        "axes[0, 0].plot(y_test.index, mejor_pred, label=f'Predicción {mejor_modelo_info[\"Orden\"]}',\n",
        "                color='red', linewidth=2, linestyle='--', marker='s', markersize=4)\n",
        "axes[0, 0].set_title(f'Predicciones {mejor_modelo_info[\"Orden\"]} vs Valores Reales', fontweight='bold', fontsize=12)\n",
        "axes[0, 0].set_xlabel('Índice')\n",
        "axes[0, 0].set_ylabel('Cantidad Vendida (kg)')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Residuos vs Tiempo\n",
        "residuos = mejor_modelo_fit.resid\n",
        "axes[0, 1].plot(residuos, color='purple', alpha=0.7)\n",
        "axes[0, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
        "axes[0, 1].fill_between(range(len(residuos)),\n",
        "                        -2*residuos.std(), 2*residuos.std(),\n",
        "                        alpha=0.2, color='gray')\n",
        "axes[0, 1].set_title('Residuos del Modelo', fontweight='bold', fontsize=12)\n",
        "axes[0, 1].set_xlabel('Observación')\n",
        "axes[0, 1].set_ylabel('Residuo')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Histograma de residuos\n",
        "axes[1, 0].hist(residuos, bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
        "axes[1, 0].axvline(residuos.mean(), color='red', linestyle='--', linewidth=2, label=f'Media: {residuos.mean():.2f}')\n",
        "axes[1, 0].set_title('Distribución de Residuos', fontweight='bold', fontsize=12)\n",
        "axes[1, 0].set_xlabel('Residuo')\n",
        "axes[1, 0].set_ylabel('Frecuencia')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 4. Q-Q Plot\n",
        "stats.probplot(residuos, dist=\"norm\", plot=axes[1, 1])\n",
        "axes[1, 1].set_title('Q-Q Plot de Residuos', fontweight='bold', fontsize=12)\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 9. GRÁFICO DE COEFICIENTES DE VARIABLES EXÓGENAS\n",
        "\n",
        "\n",
        "# Extraer solo coeficientes de variables exógenas\n",
        "exog_coef = params_df[params_df['Variable'].isin(exog_vars)].copy()\n",
        "exog_coef = exog_coef.sort_values('Coeficiente')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "colors = ['green' if p < 0.05 else 'gray' for p in exog_coef['p-value']]\n",
        "bars = plt.barh(exog_coef['Variable'], exog_coef['Coeficiente'], color=colors, alpha=0.7)\n",
        "\n",
        "# Añadir barras de error\n",
        "plt.errorbar(exog_coef['Coeficiente'], range(len(exog_coef)),\n",
        "             xerr=1.96*exog_coef['Std Error'], fmt='none', color='black', capsize=5)\n",
        "\n",
        "plt.axvline(x=0, color='red', linestyle='--', linewidth=1)\n",
        "plt.xlabel('Coeficiente', fontsize=12)\n",
        "plt.title(f'Coeficientes de Variables Exógenas - {mejor_modelo_info[\"Orden\"]}',\n",
        "          fontweight='bold', fontsize=14)\n",
        "plt.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Leyenda\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [\n",
        "    Patch(facecolor='green', alpha=0.7, label='Significativo (p < 0.05)'),\n",
        "    Patch(facecolor='gray', alpha=0.7, label='No significativo (p ≥ 0.05)')\n",
        "]\n",
        "plt.legend(handles=legend_elements, loc='best')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 10. COMPARACIÓN: MEJOR ARIMA vs MEJOR SARIMAX\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARACIÓN: ARIMA(1,0,1) vs MEJOR SARIMAX\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Datos del mejor ARIMA previo (ajustar según tus resultados)\n",
        "arima_mae = 74.06  # Del análisis anterior\n",
        "arima_mape = 42.48\n",
        "\n",
        "mejora_mae = ((arima_mae - mejor_modelo_info['MAE']) / arima_mae) * 100\n",
        "mejora_mape = ((arima_mape - mejor_modelo_info['MAPE']) / arima_mape) * 100\n",
        "\n",
        "comparacion = pd.DataFrame({\n",
        "    'Modelo': ['ARIMA(1,0,1)', mejor_modelo_info['Orden']],\n",
        "    'MAE': [arima_mae, mejor_modelo_info['MAE']],\n",
        "    'MAPE (%)': [arima_mape, mejor_modelo_info['MAPE']],\n",
        "    'Variables Exógenas': ['No', 'Sí']\n",
        "})\n",
        "\n",
        "print(comparacion.to_string(index=False))\n",
        "\n",
        "print(f\"\\n📊 Mejora respecto a ARIMA(1,0,1):\")\n",
        "print(f\"   MAE: {mejora_mae:+.2f}% {'(mejora)' if mejora_mae > 0 else '(empeora)'}\")\n",
        "print(f\"   MAPE: {mejora_mape:+.2f}% {'(mejora)' if mejora_mape > 0 else '(empeora)'}\")\n",
        "\n",
        "if mejora_mae > 0 and mejora_mape > 0:\n",
        "    print(\"\\n✅ Las variables exógenas MEJORAN significativamente el modelo\")\n",
        "elif mejora_mae < -5 or mejora_mape < -5:\n",
        "    print(\"\\n⚠️ Las variables exógenas EMPEORAN el modelo. Considerar usar ARIMA simple\")\n",
        "else:\n",
        "    print(\"\\n⚡ Las variables exógenas tienen impacto marginal. Evaluar trade-off complejidad vs mejora\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "\n",
        "# 11. MOSTRAR PRIMERAS 10 PREDICCIONES\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PRIMERAS 10 PREDICCIONES DEL MEJOR MODELO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Crear DataFrame con las primeras 10 predicciones\n",
        "n_mostrar = min(10, len(y_test))  # Por si hay menos de 10 en el conjunto de prueba\n",
        "\n",
        "df_predicciones = pd.DataFrame({\n",
        "    'Fecha/Índice': y_test.index[:n_mostrar],\n",
        "    'Valor_Real': y_test.values[:n_mostrar],\n",
        "    'Predicción': mejor_pred.values[:n_mostrar],\n",
        "    'Error': y_test.values[:n_mostrar] - mejor_pred.values[:n_mostrar],\n",
        "    'Error_Porcentual': ((y_test.values[:n_mostrar] - mejor_pred.values[:n_mostrar]) / y_test.values[:n_mostrar] * 100)\n",
        "})\n",
        "\n",
        "# Redondear para mejor visualización\n",
        "df_predicciones['Valor_Real'] = df_predicciones['Valor_Real'].round(2)\n",
        "df_predicciones['Predicción'] = df_predicciones['Predicción'].round(2)\n",
        "df_predicciones['Error'] = df_predicciones['Error'].round(2)\n",
        "df_predicciones['Error_Porcentual'] = df_predicciones['Error_Porcentual'].round(2)\n",
        "\n",
        "print(\"\\nPrimeras 10 predicciones vs valores reales:\")\n",
        "print(df_predicciones.to_string(index=False))\n",
        "\n",
        "# Estadísticas de las primeras 10 predicciones\n",
        "print(f\"\\n📊 Estadísticas de las primeras {n_mostrar} predicciones:\")\n",
        "print(f\"   Error medio: {df_predicciones['Error'].mean():.2f} kg\")\n",
        "print(f\"   Error absoluto medio: {df_predicciones['Error'].abs().mean():.2f} kg\")\n",
        "print(f\"   Error porcentual medio: {df_predicciones['Error_Porcentual'].mean():.2f}%\")\n",
        "print(f\"   MAPE primeras {n_mostrar}: {df_predicciones['Error_Porcentual'].abs().mean():.2f}%\")\n",
        "\n",
        "\n",
        "# 12. VISUALIZACIÓN DE LAS PRIMERAS 10 PREDICCIONES\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Gráfico de barras comparativo\n",
        "x_pos = np.arange(n_mostrar)\n",
        "width = 0.35\n",
        "\n",
        "bars1 = axes[0].bar(x_pos - width/2, df_predicciones['Valor_Real'],\n",
        "                     width, label='Real', color='green', alpha=0.7)\n",
        "bars2 = axes[0].bar(x_pos + width/2, df_predicciones['Predicción'],\n",
        "                     width, label='Predicción', color='red', alpha=0.7)\n",
        "\n",
        "axes[0].set_xlabel('Observación')\n",
        "axes[0].set_ylabel('Cantidad Vendida (kg)')\n",
        "axes[0].set_title(f'Primeras {n_mostrar} Predicciones vs Valores Reales', fontweight='bold')\n",
        "axes[0].set_xticks(x_pos)\n",
        "axes[0].set_xticklabels([f'{i+1}' for i in range(n_mostrar)])\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Añadir valores sobre las barras\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:.0f}',\n",
        "                    ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "# Gráfico de líneas con las primeras predicciones\n",
        "axes[1].plot(range(1, n_mostrar+1), df_predicciones['Valor_Real'],\n",
        "             marker='o', linewidth=2, markersize=8, label='Real', color='green')\n",
        "axes[1].plot(range(1, n_mostrar+1), df_predicciones['Predicción'],\n",
        "             marker='s', linewidth=2, markersize=8, label='Predicción',\n",
        "             linestyle='--', color='red')\n",
        "\n",
        "axes[1].set_xlabel('Observación')\n",
        "axes[1].set_ylabel('Cantidad Vendida (kg)')\n",
        "axes[1].set_title(f'Comparación de las Primeras {n_mostrar} Predicciones', fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Añadir área sombreada para el error\n",
        "axes[1].fill_between(range(1, n_mostrar+1),\n",
        "                     df_predicciones['Valor_Real'],\n",
        "                     df_predicciones['Predicción'],\n",
        "                     alpha=0.2, color='gray', label='Error')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 13. ANÁLISIS DE PRECISIÓN POR PREDICCIÓN\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANÁLISIS DE PRECISIÓN POR POSICIÓN DE PREDICCIÓN\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calcular métricas por posición\n",
        "df_precision = pd.DataFrame({\n",
        "    'Posición': range(1, n_mostrar+1),\n",
        "    'Error_Abs': df_predicciones['Error'].abs().values,\n",
        "    'Error_Pct_Abs': df_predicciones['Error_Porcentual'].abs().values,\n",
        "    'Subestimación': ['Sí' if e < 0 else 'No' for e in df_predicciones['Error'].values]\n",
        "})\n",
        "\n",
        "print(\"\\nError por posición de predicción:\")\n",
        "print(df_precision.to_string(index=False))\n",
        "\n",
        "# Identificar las mejores y peores predicciones\n",
        "mejor_pred_idx = df_precision['Error_Pct_Abs'].idxmin() + 1\n",
        "peor_pred_idx = df_precision['Error_Pct_Abs'].idxmax() + 1\n",
        "\n",
        "print(f\"\\n🎯 Mejor predicción: Posición {mejor_pred_idx} (Error: {df_precision['Error_Pct_Abs'].min():.2f}%)\")\n",
        "print(f\"❌ Peor predicción: Posición {peor_pred_idx} (Error: {df_precision['Error_Pct_Abs'].max():.2f}%)\")\n",
        "\n",
        "# Contar subestimaciones vs sobreestimaciones\n",
        "n_subestimaciones = (df_predicciones['Error'] < 0).sum()\n",
        "n_sobreestimaciones = (df_predicciones['Error'] > 0).sum()\n",
        "\n",
        "print(f\"\\n📈 Tendencia del modelo en las primeras {n_mostrar} predicciones:\")\n",
        "print(f\"   Subestimaciones: {n_subestimaciones} ({n_subestimaciones/n_mostrar*100:.1f}%)\")\n",
        "print(f\"   Sobreestimaciones: {n_sobreestimaciones} ({n_sobreestimaciones/n_mostrar*100:.1f}%)\")\n",
        "\n",
        "if n_subestimaciones > n_sobreestimaciones:\n",
        "    print(\"   → El modelo tiende a SUBESTIMAR las ventas\")\n",
        "elif n_sobreestimaciones > n_subestimaciones:\n",
        "    print(\"   → El modelo tiende a SOBREESTIMAR las ventas\")\n",
        "else:\n",
        "    print(\"   → El modelo está balanceado entre subestimaciones y sobreestimaciones\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# 14. PRONÓSTICO FUTURO (próximas 4 semanas) con exógenas\n",
        "\n",
        "\n",
        "# 14.1: construir exógenas futuras (4 filas, mismo orden/columnas que exog_vars)\n",
        "def build_future_exog(df_full, exog_cols, steps=4):\n",
        "    \"\"\"\n",
        "    Crea X_future con 4 filas usando estrategia simple:\n",
        "    - forward-fill de últimas observaciones para numéricos\n",
        "    - si tienes feriados (Es_Feriado / Pre_Feriado / Post_Feriado) puedes\n",
        "      ajustarlos manualmente aquí según tu calendario\n",
        "    \"\"\"\n",
        "    last_rows = df_full[exog_cols].copy().tail(8)       # base de referencia\n",
        "    X_future = pd.DataFrame(columns=exog_cols)\n",
        "\n",
        "    # estrategia por defecto: mantener último valor (o promedio reciente)\n",
        "    last_vals = last_rows.iloc[-1:].copy()\n",
        "    mean_vals = last_rows.mean(numeric_only=True)\n",
        "\n",
        "    for i in range(steps):\n",
        "        row = last_vals.copy()\n",
        "        # ejemplo alternativo: usa promedio reciente para suavizar\n",
        "        # row.loc[:, :] = mean_vals.values\n",
        "\n",
        "        # si tienes reglas para feriados, ajústalas aquí:\n",
        "        # row['Es_Feriado'] = 0\n",
        "        # row['Pre_Feriado'] = 0\n",
        "        # row['Post_Feriado'] = 0\n",
        "\n",
        "        X_future = pd.concat([X_future, row], ignore_index=True)\n",
        "\n",
        "    # asegurar tipos numéricos y orden de columnas\n",
        "    X_future = X_future[exog_cols].apply(pd.to_numeric, errors='coerce').fillna(method='ffill').fillna(0)\n",
        "    return X_future\n",
        "\n",
        "# crea X_future (4 semanas)\n",
        "X_future = build_future_exog(df_winsorizado, exog_vars, steps=4)\n",
        "\n",
        "# 14.2: pronóstico 4 pasos adelante\n",
        "y_future_4w = mejor_modelo_fit.forecast(steps=4, exog=X_future)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PRONÓSTICO: Próximas 4 semanas (SARIMAX con exógenas)\")\n",
        "print(\"=\"*80)\n",
        "print(pd.Series(y_future_4w).round(2))\n",
        "\n",
        "# 14.3: armar índice temporal futuro usando ÍNDICE numérico\n",
        "steps = 4  # por claridad\n",
        "base_idx = len(df_winsorizado) - 1              # último índice real (p.ej., 246)\n",
        "future_index = np.arange(base_idx + 1, base_idx + steps + 1)  # 247..250\n",
        "\n",
        "# Tabla para el plot futuro\n",
        "df_future_plot = pd.DataFrame({'Semana': future_index, 'Predicción': np.asarray(y_future_4w)})\n",
        "\n",
        "# Histórico con el mismo eje (0..N-1)\n",
        "x_hist = pd.DataFrame({\n",
        "    'Semana': np.arange(len(df_winsorizado)),\n",
        "    'Cantidad_Vendida': df_winsorizado['Cantidad_Vendida'].values\n",
        "})\n",
        "\n",
        "x_start_line = future_index[0]   # 247\n",
        "x_span_end   = future_index[-1]  # 250\n",
        "\n",
        "# 14.4: gráfico estilo SARIMAX con paleta 'rocket' (índice numérico)\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"talk\")\n",
        "palette = sns.color_palette(\"rocket\", 6)\n",
        "c_hist = palette[4]\n",
        "c_pred = palette[1]\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "\n",
        "# Histórico\n",
        "plt.plot(x_hist['Semana'], x_hist['Cantidad_Vendida'],\n",
        "         color=c_hist, linewidth=1.8, alpha=0.7, label='Entrenamiento (real)')\n",
        "\n",
        "# Futuro\n",
        "plt.plot(df_future_plot['Semana'], df_future_plot['Predicción'],\n",
        "         color=c_pred, linestyle='--', marker='s', linewidth=2.2, markersize=4,\n",
        "         label='Pronóstico 4 semanas')\n",
        "\n",
        "# Línea vertical y sombreado\n",
        "plt.axvline(x=x_start_line, color='tab:red', linestyle=':', linewidth=2, label='Inicio Pronóstico')\n",
        "plt.axvspan(x_start_line, x_span_end, color='gray', alpha=0.10)\n",
        "\n",
        "plt.title(\"Pronóstico SARIMAX (próximas 4 semanas)\", fontsize=16, fontweight='bold')\n",
        "plt.xlabel(\"Semana (índice)\")\n",
        "plt.ylabel(\"Cantidad Vendida (kg)\")\n",
        "plt.legend(frameon=True, loc='upper left')\n",
        "plt.grid(True, linestyle='--', alpha=0.3)\n",
        "sns.despine()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KUxQeBFqALWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# APLICAR ESTILO SEABORN ROCKSTAR\n",
        "\n",
        "\n",
        "# Establecer el estilo de seaborn\n",
        "sns.set_style(\"whitegrid\")  # o \"darkgrid\", \"white\", \"dark\", \"ticks\"\n",
        "\n",
        "# Usar la paleta de colores Rocket de seaborn\n",
        "colores_rocket = sns.color_palette(\"rocket\", n_colors=5)\n",
        "\n",
        "\n",
        "# GRÁFICO CON COLORES ROCKET\n",
        "\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Datos de entrenamiento - usar primer color de rocket\n",
        "plt.plot(y_train.index, y_train,\n",
        "         label='Entrenamiento',\n",
        "         color=colores_rocket[0],  # Color rocket\n",
        "         alpha=0.7,\n",
        "         linewidth=1.5)\n",
        "\n",
        "# Valores reales de test - usar segundo color\n",
        "plt.plot(y_test.index, y_test,\n",
        "         label='Real (Test)',\n",
        "         color=colores_rocket[2],  # Color rocket\n",
        "         linewidth=2.5,\n",
        "         marker='o',\n",
        "         markersize=4)\n",
        "\n",
        "# Predicciones - usar tercer color\n",
        "plt.plot(y_test.index, mejor_pred,\n",
        "         label=f'Predicción {mejor_modelo_info[\"Orden\"]}',\n",
        "         color=colores_rocket[4],  # Color rocket\n",
        "         linewidth=2.5,\n",
        "         linestyle='--',\n",
        "         marker='s',\n",
        "         markersize=4)\n",
        "\n",
        "# Línea vertical separando train/test\n",
        "plt.axvline(x=y_train.index[-1],\n",
        "            color=colores_rocket[3],  # Color rocket para la línea\n",
        "            linestyle=':',\n",
        "            linewidth=2,\n",
        "            alpha=0.7,\n",
        "            label='Inicio Predicción')\n",
        "\n",
        "plt.title(f'Predicciones {mejor_modelo_info[\"Orden\"]} vs Valores Reales',\n",
        "          fontweight='bold',\n",
        "          fontsize=14)\n",
        "plt.xlabel('Índice de Observación', fontsize=12)\n",
        "plt.ylabel('Cantidad Vendida (kg)', fontsize=12)\n",
        "plt.legend(loc='best', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eyXH_PkUGZZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wausxLSanYph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODELO XGBOOST**"
      ],
      "metadata": {
        "id": "xWJ_YvJ9EtRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_winsorizado.info()"
      ],
      "metadata": {
        "id": "lHn-a_r9Jbx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_winsorizado.head()"
      ],
      "metadata": {
        "id": "QLcjEUhr6Eyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_winsorizado.describe()"
      ],
      "metadata": {
        "id": "7PIsp0X86Vj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "def plot_predictions(df, y_train, y_test, y_pred):\n",
        "    \"\"\"Genera gráfico de entrenamiento, prueba y predicciones.\"\"\"\n",
        "    # Crear un índice temporal coherente (puede ser fecha o número de semana)\n",
        "    train_index = df.iloc[:len(y_train)].index\n",
        "    test_index = df.iloc[len(y_train):].index\n",
        "\n",
        "    plt.figure(figsize=(14,6))\n",
        "    plt.plot(train_index, y_train, label=\"Entrenamiento (real)\", color=\"gray\", alpha=0.6)\n",
        "    plt.plot(test_index, y_test, label=\"Prueba (real)\", color=\"blue\", linewidth=2)\n",
        "    plt.plot(test_index, y_pred, label=\"Predicción XGBoost\", color=\"red\", linestyle=\"--\", linewidth=2)\n",
        "\n",
        "    plt.title(\"Predicción de Cantidad Vendida - XGBoost\", fontsize=14)\n",
        "    plt.xlabel(\"Índice temporal o semana\")\n",
        "    plt.ylabel(\"Cantidad vendida\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def run_model(df):\n",
        "    print(\"=\"*70)\n",
        "    print(\"MODELO XGBOOST - PREDICCIÓN DE CANTIDAD VENDIDA\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Excluir variables no predictoras\n",
        "    exclude_cols = [\"Cantidad_Vendida\", \"Año_Semana\"]\n",
        "    X = df.drop(columns=[col for col in exclude_cols if col in df.columns])\n",
        "    y = df[\"Cantidad_Vendida\"]\n",
        "\n",
        "    # Asegurar tipo numérico\n",
        "    X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
        "    X = X.fillna(0)\n",
        "\n",
        "    # División de datos (mantiene orden temporal)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    print(\"Datos de entrenamiento:\", X_train.shape)\n",
        "    print(\"Datos de prueba:       \", X_test.shape)\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    # Crear y entrenar modelo\n",
        "    model = xgb.XGBRegressor(\n",
        "        objective=\"reg:squarederror\",\n",
        "        n_estimators=300,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=6,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predicciones\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluación\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(\"Métricas del modelo:\")\n",
        "    print(f\"MAE:  {mae:.3f}\")\n",
        "    print(f\"RMSE: {rmse:.3f}\")\n",
        "    print(f\"R²:   {r2:.3f}\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    # Comparación de valores reales vs predichos\n",
        "    comparison = pd.DataFrame({\n",
        "        \"Real\": y_test.values,\n",
        "        \"Predicho\": y_pred\n",
        "    }).reset_index(drop=True)\n",
        "\n",
        "    print(\"Ejemplo de predicciones:\")\n",
        "    print(comparison.head(10))\n",
        "\n",
        "    # Generar gráfico\n",
        "    plot_predictions(df, y_train, y_test, y_pred)\n",
        "\n",
        "        # Ver estructura del booster\n",
        "    booster = model.get_booster()\n",
        "\n",
        "    # Parámetros del modelo\n",
        "    print(\"Parámetros del modelo:\")\n",
        "    print(model.get_params())\n",
        "\n",
        "    # Número de árboles\n",
        "    print(\"Número de árboles:\", len(booster.get_dump()))\n",
        "\n",
        "    # Número total de nodos (≈ parámetros)\n",
        "    num_nodes = sum([tree.count('\\n') for tree in booster.get_dump()])\n",
        "    print(\"Número total de nodos (≈ parámetros):\", num_nodes)\n",
        "\n",
        "    # Importancia de características\n",
        "    importances = model.feature_importances_\n",
        "    features = X_train.columns\n",
        "\n",
        "    print(\"\\nImportancia de características:\")\n",
        "    for f, imp in zip(features, importances):\n",
        "        print(f\"{f}: {imp:.4f}\")\n",
        "\n",
        "    return model, comparison, y_train, y_test, y_pred, X_train, X_test\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wch4ED1RClWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = run_model(df_winsorizado)"
      ],
      "metadata": {
        "id": "z4H5fTKX_G1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# 1) Helpers de calendario\n",
        "\n",
        "def _ensure_fecha(df):\n",
        "    df = df.copy()\n",
        "    if 'Fecha' in df.columns and np.issubdtype(df['Fecha'].dtype, np.datetime64):\n",
        "        return df\n",
        "    # Si no hay Fecha, intentamos construirla desde Año y Semana_Num (lunes de esa ISO week)\n",
        "    if {'Año','Semana_Num'}.issubset(df.columns):\n",
        "        # Usamos ISO week: lunes de la semana\n",
        "        df['Fecha'] = pd.to_datetime(df['Año'].astype(int).astype(str) + '-1', format='%G-%u') \\\n",
        "                        + pd.to_timedelta((df['Semana_Num'].astype(int)-1)*7, unit='D')\n",
        "        return df\n",
        "    # Si existe Año_Semana tipo \"YYYY-ww\"\n",
        "    if 'Año_Semana' in df.columns:\n",
        "        parts = df['Año_Semana'].astype(str).str.split('-', expand=True)\n",
        "        if parts.shape[1] == 2:\n",
        "            año = parts[0].astype(int)\n",
        "            semana = parts[1].astype(int)\n",
        "            df['Fecha'] = pd.to_datetime(año.astype(str) + '-1', format='%G-%u') \\\n",
        "                          + pd.to_timedelta((semana-1)*7, unit='D')\n",
        "            return df\n",
        "    raise ValueError(\"No se pudo determinar 'Fecha'. Proporciona 'Fecha' o ('Año','Semana_Num') o 'Año_Semana'.\")\n",
        "\n",
        "def _next_year_week(year:int, week:int):\n",
        "    # Avanza una semana en calendario ISO (simplificado: 52->1, 53->1)\n",
        "    week += 1\n",
        "    if week > 53:  # por si hay año con semana 53\n",
        "        week = 1\n",
        "        year += 1\n",
        "    if week > 52:  # mayoría de años\n",
        "        # dejamos 53 por si tu data tiene esa semana; si no, lo mandamos a 1\n",
        "        week = 1\n",
        "        year += 1\n",
        "    return year, week\n",
        "\n",
        "\n",
        "# 2) Feature engineering\n",
        "\n",
        "def build_features(df_raw):\n",
        "    \"\"\"\n",
        "    Devuelve df con features consistentes para train y forecast.\n",
        "    - Requiere 'Fecha' (datetime) y 'Cantidad_Vendida', 'Inventario', 'Mermas', 'Cantidad_Producida' (si existen).\n",
        "    \"\"\"\n",
        "    df = _ensure_fecha(df_raw).copy()\n",
        "    df = df.sort_values('Fecha').reset_index(drop=True)\n",
        "\n",
        "    # Variables temporales\n",
        "    df['Mes'] = df['Fecha'].dt.month\n",
        "    df['Trimestre'] = df['Fecha'].dt.quarter\n",
        "    df['Dia_Semana'] = df['Fecha'].dt.dayofweek\n",
        "    # Semana ISO (número)\n",
        "    df['Semana_Año'] = df['Fecha'].dt.isocalendar().week.astype(int)\n",
        "    df['Año'] = df['Fecha'].dt.isocalendar().year.astype(int)\n",
        "    # Tendencia simple\n",
        "    df['Tendencia'] = np.arange(len(df))\n",
        "\n",
        "    # Lags (si existe Cantidad_Vendida)\n",
        "    if 'Cantidad_Vendida' in df.columns:\n",
        "        for lag in [1,2,3,4]:\n",
        "            df[f'Lag_{lag}'] = df['Cantidad_Vendida'].shift(lag)\n",
        "        # Medias móviles basadas en rezagos\n",
        "        df['MA_3'] = df['Cantidad_Vendida'].shift(1).rolling(3).mean()\n",
        "        df['MA_6'] = df['Cantidad_Vendida'].shift(1).rolling(6).mean()\n",
        "\n",
        "    # Ratios si hay columnas\n",
        "    if {'Inventario','Cantidad_Vendida'}.issubset(df.columns):\n",
        "        df['Ratio_Inv_Venta'] = df['Inventario'] / (df['Cantidad_Vendida'] + 1)\n",
        "    else:\n",
        "        df['Ratio_Inv_Venta'] = np.nan\n",
        "    if {'Mermas','Cantidad_Producida'}.issubset(df.columns):\n",
        "        df['Ratio_Mermas'] = df['Mermas'] / (df['Cantidad_Producida'] + 1)\n",
        "    else:\n",
        "        df['Ratio_Mermas'] = np.nan\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# 3) Entrenar SIN la última semana y guardar columnas\n",
        "\n",
        "def train_excluding_last(df_raw):\n",
        "    df_feat = build_features(df_raw)\n",
        "\n",
        "    # Excluir la última fila REAL del entrenamiento\n",
        "    df_train = df_feat.iloc[:-1].copy()\n",
        "\n",
        "    # Columnas a excluir SI existen\n",
        "    exclude_cols = {'Cantidad_Vendida','Fecha','Año_Semana','Ventas_Total'}\n",
        "    # Construimos X e y\n",
        "    feature_cols = [c for c in df_train.columns if c not in exclude_cols]\n",
        "    X_train = df_train[feature_cols].select_dtypes(include=[np.number]).fillna(0)\n",
        "    y_train = df_train['Cantidad_Vendida']\n",
        "\n",
        "    # Guardamos exactamente el listado de columnas numéricas usadas\n",
        "    feature_cols_num = list(X_train.columns)\n",
        "\n",
        "    # Modelo\n",
        "    model = xgb.XGBRegressor(\n",
        "        objective=\"reg:squarederror\",\n",
        "        n_estimators=400,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=6,\n",
        "        subsample=0.9,\n",
        "        colsample_bytree=0.9,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    return model, feature_cols_num, df_feat\n",
        "\n",
        "\n",
        "# 4) Forecast iterativo 4 semanas\n",
        "\n",
        "def forecast_next_4_weeks_excluding_last(df_raw, weeks_ahead=4):\n",
        "    model, feature_cols_num, df_feat = train_excluding_last(df_raw)\n",
        "\n",
        "    # Partimos del df_feat ORIGINAL (incluye la última real), y vamos agregando semanas futuras\n",
        "    df_forecast = df_feat.copy()\n",
        "\n",
        "    preds = []\n",
        "    for _ in range(weeks_ahead):\n",
        "        # 4.1) Crear la fila \"placeholder\" de la próxima semana\n",
        "        last_fecha = df_forecast['Fecha'].iloc[-1]\n",
        "        next_fecha = last_fecha + pd.Timedelta(days=7)\n",
        "\n",
        "        new_row = {col: np.nan for col in df_forecast.columns}\n",
        "        new_row['Fecha'] = next_fecha\n",
        "\n",
        "        # Si tienes Año y Semana_Num, actualízalos también (opcional)\n",
        "        if {'Año','Semana_Num'}.issubset(df_forecast.columns):\n",
        "            last_year = int(df_forecast['Año'].iloc[-1])\n",
        "            last_week = int(df_forecast['Semana_Num'].iloc[-1]) if 'Semana_Num' in df_forecast.columns else int(df_forecast['Semana_Año'].iloc[-1])\n",
        "            ny, nw = _next_year_week(last_year, last_week)\n",
        "            new_row['Año'] = ny\n",
        "            new_row['Semana_Num'] = nw\n",
        "\n",
        "        # Añadir la fila vacía\n",
        "        df_forecast = pd.concat([df_forecast, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "        # 4.2) Recalcular features en TODO el df (ahora incluye la fila futura)\n",
        "        df_forecast = build_features(df_forecast)\n",
        "\n",
        "        # 4.3) Armar X_new con EXACTAS columnas usadas en el entrenamiento\n",
        "        X_new = df_forecast.iloc[[-1]][feature_cols_num].select_dtypes(include=[np.number]).fillna(0)\n",
        "\n",
        "        # 4.4) Predecir y asignar\n",
        "        y_hat = float(model.predict(X_new)[0])\n",
        "        preds.append({'Fecha': df_forecast['Fecha'].iloc[-1], 'Prediccion': y_hat})\n",
        "        # Escribimos la predicción como Cantidad_Vendida para que la siguiente iteración tenga lags correctos\n",
        "        df_forecast.loc[df_forecast.index[-1], 'Cantidad_Vendida'] = y_hat\n",
        "\n",
        "    pred_df = pd.DataFrame(preds)\n",
        "    # Si quieres Año-Semana legible:\n",
        "    pred_df['Año_Semana'] = pred_df['Fecha'].dt.isocalendar().year.astype(str) + \"-\" + \\\n",
        "                            pred_df['Fecha'].dt.isocalendar().week.astype(str).str.zfill(2)\n",
        "    return model, pred_df, feature_cols_num\n",
        "\n",
        "\n",
        "# 5) (Opcional) Métricas de backtest con la cola real\n",
        "\n",
        "def backtest_last_block(df_raw, model, feature_cols_num, test_size=8):\n",
        "    \"\"\"Evalúa el modelo en las últimas 'test_size' semanas previas a la última real utilizada.\"\"\"\n",
        "    df_feat = build_features(df_raw).dropna().reset_index(drop=True)\n",
        "    if len(df_feat) <= test_size+1:\n",
        "        print(\"Serie muy corta para backtest.\")\n",
        "        return None\n",
        "\n",
        "    train_idx = len(df_feat) - test_size - 1  # -1 para excluir la última real del entrenamiento original\n",
        "    df_train = df_feat.iloc[:train_idx]\n",
        "    df_test  = df_feat.iloc[train_idx: -1]    # hasta la penúltima real\n",
        "\n",
        "    X_test = df_test[feature_cols_num].select_dtypes(include=[np.number]).fillna(0)\n",
        "    y_test = df_test['Cantidad_Vendida']\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(f\"Backtest últimas {test_size} semanas -> MAE: {mae:.2f} | RMSE: {rmse:.2f} | R²: {r2:.3f}\")\n",
        "    return pd.DataFrame({'Fecha': df_test['Fecha'], 'Real': y_test.values, 'Predicho': y_pred})\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FIXY59weEHE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, pred_4w, feature_cols = forecast_next_4_weeks_excluding_last(df_winsorizado, weeks_ahead=4)\n",
        "print(pred_4w)\n"
      ],
      "metadata": {
        "id": "muG9rioYEJIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def _ensure_fecha_col(df):\n",
        "    df = df.copy()\n",
        "    if 'Fecha' in df.columns and np.issubdtype(df['Fecha'].dtype, np.datetime64):\n",
        "        return df\n",
        "    if 'Año_Semana' in df.columns:\n",
        "        parts = df['Año_Semana'].astype(str).str.split('-', expand=True)\n",
        "        if parts.shape[1] == 2:\n",
        "            year = parts[0].astype(int)\n",
        "            week = parts[1].astype(int)\n",
        "            df['Fecha'] = pd.to_datetime(year.astype(str) + '-1', format='%G-%u') \\\n",
        "                          + pd.to_timedelta((week-1)*7, unit='D')\n",
        "            return df\n",
        "    raise ValueError(\"Proporciona 'Fecha' (datetime) o 'Año_Semana' (YYYY-ww).\")\n",
        "\n",
        "def plot_history_with_forecast(df_hist, df_fore, y_col_hist='Cantidad_Vendida', y_col_fore='Prediccion'):\n",
        "    df_hist = _ensure_fecha_col(df_hist)\n",
        "    df_fore = _ensure_fecha_col(df_fore)\n",
        "\n",
        "    plt.figure(figsize=(13,6))\n",
        "    # línea histórico\n",
        "    plt.plot(df_hist['Fecha'], df_hist[y_col_hist], label='Histórico (real)', linewidth=2)\n",
        "    # línea forecast\n",
        "    plt.plot(df_fore['Fecha'], df_fore[y_col_fore], label='Pronóstico (4 semanas)', linestyle='--', linewidth=2, marker='o')\n",
        "\n",
        "    # sombreado del período futuro\n",
        "    start_shade = df_fore['Fecha'].min()\n",
        "    end_shade = df_fore['Fecha'].max()\n",
        "    plt.axvspan(start_shade, end_shade, color='gray', alpha=0.15, label='Ventana de pronóstico')\n",
        "\n",
        "    # anotaciones de puntos (opcional)\n",
        "    for x, y in zip(df_fore['Fecha'], df_fore[y_col_fore]):\n",
        "        plt.annotate(f\"{y:.0f}\", (x, y), textcoords=\"offset points\", xytext=(0,8), ha='center', fontsize=9)\n",
        "\n",
        "    plt.title('Histórico + Pronóstico 4 semanas', fontsize=14)\n",
        "    plt.xlabel('Fecha')\n",
        "    plt.ylabel('Cantidad vendida')\n",
        "    plt.grid(True, linestyle='--', alpha=0.4)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# USO:\n",
        "# plot_history_with_forecast(df_winsorizado, pred_4w)\n"
      ],
      "metadata": {
        "id": "y3HUiswpFBAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history_with_forecast(df_winsorizado, pred_4w)"
      ],
      "metadata": {
        "id": "c-ElNuXYFGQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_feature_importance(model, feature_cols, top=15):\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    importances = model.feature_importances_\n",
        "    order = np.argsort(importances)[::-1][:top]\n",
        "    feats = np.array(feature_cols)[order]\n",
        "    vals = importances[order]\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.barh(range(len(feats)), vals[::-1])\n",
        "    plt.yticks(range(len(feats)), feats[::-1])\n",
        "    plt.xlabel('Importancia')\n",
        "    plt.title(f'Importancia de variables (Top {top})')\n",
        "    plt.grid(True, axis='x', linestyle='--', alpha=0.4)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# USO:\n",
        "plot_feature_importance(model, feature_cols)\n"
      ],
      "metadata": {
        "id": "NhSctgqQFO3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_xgboost_importance_all(model, top_n=15):\n",
        "    \"\"\"\n",
        "    Visualiza las importancias de variables en un modelo XGBoost usando\n",
        "    las métricas: weight, gain y cover. Usa Seaborn con paleta 'rocket'.\n",
        "    \"\"\"\n",
        "    sns.set_theme(style=\"whitegrid\", palette=\"rocket\")\n",
        "\n",
        "    # Obtener booster\n",
        "    booster = model.get_booster()\n",
        "    importance_types = ['weight', 'gain', 'cover']\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "    for idx, imp_type in enumerate(importance_types):\n",
        "        # Obtener importancia\n",
        "        importance_dict = booster.get_score(importance_type=imp_type)\n",
        "\n",
        "        # Convertir a DataFrame ordenado\n",
        "        importance_df = (\n",
        "            pd.DataFrame(list(importance_dict.items()), columns=['Feature', 'Importance'])\n",
        "            .sort_values('Importance', ascending=False)\n",
        "            .head(top_n)\n",
        "        )\n",
        "\n",
        "        # Graficar\n",
        "        ax = axes[idx]\n",
        "        sns.barplot(\n",
        "            data=importance_df,\n",
        "            y='Feature',\n",
        "            x='Importance',\n",
        "            ax=ax,\n",
        "            palette=\"rocket\"\n",
        "        )\n",
        "        ax.set_title(f\"Top {top_n} Features - {imp_type.capitalize()}\", fontsize=12, fontweight='bold')\n",
        "        ax.set_xlabel('Importancia')\n",
        "        ax.set_ylabel('')\n",
        "        ax.invert_yaxis()\n",
        "\n",
        "        # Anotar valores\n",
        "        for i, val in enumerate(importance_df['Importance']):\n",
        "            ax.text(val, i, f'{val:.2f}', va='center', fontsize=9)\n",
        "\n",
        "    plt.suptitle(\"Importancia de Variables en XGBoost (weight, gain, cover)\", fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "cph09GWXYA6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_xgboost_importance_all(model, top_n=15)"
      ],
      "metadata": {
        "id": "Gt0TOxrEYCcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_forecast_table(pred_4w, round_to=2, export_csv=None):\n",
        "    df = pred_4w.copy()\n",
        "    if 'Prediccion' in df.columns:\n",
        "        df['Prediccion'] = df['Prediccion'].round(round_to)\n",
        "    if export_csv:\n",
        "        df.to_csv(export_csv, index=False)\n",
        "        print(f\"✅ Exportado a {export_csv}\")\n",
        "    return df\n",
        "\n",
        "# USO:\n",
        "show_forecast_table(pred_4w, round_to=1, export_csv='forecast_4w.csv')\n"
      ],
      "metadata": {
        "id": "--7mzWQ1F2Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ejemplo de carga (opcional)\n",
        "data = {\n",
        "    \"REAL\":[195.27,218.20,39.66,274.90,312.23,212.29,146.72,193.56,119.99,202.17],\n",
        "    \"Prediccion_ARIMA\":[157.04,156.70,156.36,156.04,155.72,155.40,155.09,154.79,154.50,154.21],\n",
        "    \"Predicción SARIMAX\":[213.82,151.45,134.33,116.25,206.63,174.15,182.48,163.95,172.08,161.32],\n",
        "    \"XGBOOST\":[172.06,208.64,38.92,261.60,312.89,222.57,162.02,192.18,150.10,257.32]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Configuración visual\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"talk\")\n",
        "sns.set_palette(\"rocket\")\n",
        "\n",
        "# 1) Serie temporal: REAL vs modelos\n",
        "def plot_series(df, save=None):\n",
        "    idx = np.arange(len(df))\n",
        "    plt.figure(figsize=(14,5))\n",
        "    plt.plot(idx, df[\"REAL\"], label=\"Real\", linewidth=2.5)\n",
        "    plt.plot(idx, df[\"Prediccion_ARIMA\"], \"--\", marker=\"o\", label=\"ARIMA\")\n",
        "    plt.plot(idx, df[\"Predicción SARIMAX\"], \"--\", marker=\"s\", label=\"SARIMAX\")\n",
        "    plt.plot(idx, df[\"XGBOOST\"], \"--\", marker=\"^\", label=\"XGBoost\")\n",
        "    plt.title(\"Comparación temporal: Real vs Predicho\")\n",
        "    plt.xlabel(\"Índice de observación\")\n",
        "    plt.ylabel(\"Cantidad vendida\")\n",
        "    plt.legend(ncol=4, frameon=True)\n",
        "    plt.grid(alpha=0.3, linestyle=\"--\")\n",
        "    plt.tight_layout()\n",
        "    if save: plt.savefig(save, dpi=150, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "\n",
        "# 2) Errores absolutos por observación\n",
        "def plot_abs_errors(df, save=None):\n",
        "    err = pd.DataFrame({\n",
        "        \"Obs\": np.arange(len(df)),\n",
        "        \"ARIMA\": (df[\"REAL\"]-df[\"Prediccion_ARIMA\"]).abs(),\n",
        "        \"SARIMAX\": (df[\"REAL\"]-df[\"Predicción SARIMAX\"]).abs(),\n",
        "        \"XGBoost\": (df[\"REAL\"]-df[\"XGBOOST\"]).abs()\n",
        "    })\n",
        "    err_melt = err.melt(id_vars=\"Obs\", var_name=\"Modelo\", value_name=\"Error absoluto\")\n",
        "    plt.figure(figsize=(14,5))\n",
        "    sns.barplot(data=err_melt, x=\"Obs\", y=\"Error absoluto\", hue=\"Modelo\")\n",
        "    plt.title(\"Error absoluto por observación y modelo\")\n",
        "    plt.xlabel(\"Índice de observación\"); plt.ylabel(\"Error absoluto\")\n",
        "    plt.legend(frameon=True)\n",
        "    plt.grid(alpha=0.3, axis=\"y\")\n",
        "    plt.tight_layout()\n",
        "    if save: plt.savefig(save, dpi=150, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "\n",
        "# 3) Dispersión Real vs Predicho (con 45°)\n",
        "def plot_scatter(df, save=None):\n",
        "    fig, ax = plt.subplots(1,3, figsize=(15,4.8), sharex=True, sharey=True)\n",
        "    modelos = [\n",
        "        (\"ARIMA\", df[\"Prediccion_ARIMA\"]),\n",
        "        (\"SARIMAX\", df[\"Predicción SARIMAX\"]),\n",
        "        (\"XGBoost\", df[\"XGBOOST\"]),\n",
        "    ]\n",
        "    lim_min = min(df[\"REAL\"].min(), df[[\"Prediccion_ARIMA\",\"Predicción SARIMAX\",\"XGBOOST\"]].min().min())\n",
        "    lim_max = max(df[\"REAL\"].max(), df[[\"Prediccion_ARIMA\",\"Predicción SARIMAX\",\"XGBOOST\"]].max().max())\n",
        "    for i,(name,yhat) in enumerate(modelos):\n",
        "        ax[i].scatter(df[\"REAL\"], yhat, s=50, alpha=0.8)\n",
        "        ax[i].plot([lim_min, lim_max], [lim_min, lim_max], \"k--\", linewidth=1)  # línea 45°\n",
        "        ax[i].set_title(name)\n",
        "        ax[i].set_xlabel(\"Real\")\n",
        "        if i==0: ax[i].set_ylabel(\"Predicho\")\n",
        "        ax[i].grid(alpha=0.3, linestyle=\"--\")\n",
        "    fig.suptitle(\"Real vs Predicho (línea ideal 45°)\", y=1.02, fontsize=18)\n",
        "    plt.tight_layout()\n",
        "    if save: plt.savefig(save, dpi=150, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "\n",
        "#  4) Distribución de errores (histogramas)\n",
        "def plot_residuals(df, save=None):\n",
        "    resid = pd.DataFrame({\n",
        "        \"ARIMA\": df[\"REAL\"]-df[\"Prediccion_ARIMA\"],\n",
        "        \"SARIMAX\": df[\"REAL\"]-df[\"Predicción SARIMAX\"],\n",
        "        \"XGBoost\": df[\"REAL\"]-df[\"XGBOOST\"]\n",
        "    })\n",
        "    resid_melt = resid.melt(var_name=\"Modelo\", value_name=\"Residual\")\n",
        "    plt.figure(figsize=(12,5))\n",
        "    sns.histplot(data=resid_melt, x=\"Residual\", hue=\"Modelo\", kde=True, bins=15, element=\"step\")\n",
        "    plt.title(\"Distribución de residuales por modelo\")\n",
        "    plt.xlabel(\"Error (Real - Predicho)\")\n",
        "    plt.grid(alpha=0.3, axis=\"y\")\n",
        "    plt.tight_layout()\n",
        "    if save: plt.savefig(save, dpi=150, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "\n",
        "#5) Tabla de métricas\n",
        "def metrics_table(df):\n",
        "    def mape(y, yhat):\n",
        "        y = np.asarray(y); yhat = np.asarray(yhat)\n",
        "        return np.mean(np.abs((y - yhat) / np.clip(y, 1e-6, None))) * 100\n",
        "    met = pd.DataFrame({\n",
        "        \"Modelo\":[\"ARIMA\",\"SARIMAX\",\"XGBoost\"],\n",
        "        \"MAE\":[\n",
        "            np.mean(np.abs(df[\"REAL\"]-df[\"Prediccion_ARIMA\"])),\n",
        "            np.mean(np.abs(df[\"REAL\"]-df[\"Predicción SARIMAX\"])),\n",
        "            np.mean(np.abs(df[\"REAL\"]-df[\"XGBOOST\"])),\n",
        "        ],\n",
        "        \"RMSE\":[\n",
        "            np.sqrt(np.mean((df[\"REAL\"]-df[\"Prediccion_ARIMA\"])**2)),\n",
        "            np.sqrt(np.mean((df[\"REAL\"]-df[\"Predicción SARIMAX\"])**2)),\n",
        "            np.sqrt(np.mean((df[\"REAL\"]-df[\"XGBOOST\"])**2)),\n",
        "        ],\n",
        "        \"MAPE (%)\":[\n",
        "            mape(df[\"REAL\"], df[\"Prediccion_ARIMA\"]),\n",
        "            mape(df[\"REAL\"], df[\"Predicción SARIMAX\"]),\n",
        "            mape(df[\"REAL\"], df[\"XGBOOST\"]),\n",
        "        ]\n",
        "    }).sort_values(\"MAE\")\n",
        "    return met\n",
        "\n",
        "\n",
        "plot_series(df)\n",
        "plot_abs_errors(df)\n",
        "plot_scatter(df)\n",
        "plot_residuals(df)\n",
        "\n",
        "print(\"Métricas resumidas:\")\n",
        "print(metrics_table(df).round(2).to_string(index=False))\n"
      ],
      "metadata": {
        "id": "9I1_wPUd-ZA3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}